{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8b992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run jupyterutils.py\n",
    "\n",
    "from algorithms.unsupervised.softmax.SoftMax import SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9726de96",
   "metadata": {},
   "source": [
    "### Softmax model with multiple feature\n",
    "We attempt to try and mimic linear regression and than test it , to test our implementations correctness we would generate the data set using a perfect linear equation , we will than compare how good our algorithm does when it get's data fitting a curve whose mathematically model is known. Run time and correctness might be good KPI for monitoring .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6328b",
   "metadata": {},
   "source": [
    "##### Create data\n",
    "\n",
    "We'll check how good is the algorithm in finding the actual parameters when we feed it absolutely ideal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2277fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelEquation(x1,x2):\n",
    "    return (9.8*x1 + 3.2*x2 + 7.6)%5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f636a270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = (9.8⋅x₁ + 3.2⋅x₂ + 2.6) mod 5\n"
     ]
    }
   ],
   "source": [
    "x1 = sp.Symbol('x1')\n",
    "x2 = sp.Symbol('x2')\n",
    "print(\"y = \", end='')\n",
    "sp.pprint(ModelEquation(x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea358307",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_points = 50000\n",
    "rng = np.random.default_rng(0)             # reproducible\n",
    "x1 = rng.uniform(0, 2, total_data_points)  # U(0,2)\n",
    "x2 = rng.uniform(0, 2, total_data_points)  # completely separate draw\n",
    "y = ModelEquation(x1,x2)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# perm = np.random.permutation(len(x1))\n",
    "# x1 = x1[perm]\n",
    "# x2 = x2[perm]\n",
    "# y = y[perm]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044f62c7",
   "metadata": {},
   "source": [
    "##### Split data\n",
    "\n",
    "We're going to split the data into training and testing data based on a tuneable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a43ab785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42500, 2)\n",
      "(7500, 2)\n",
      "(42500,)\n",
      "(7500,)\n"
     ]
    }
   ],
   "source": [
    "training_percent = 0.85\n",
    "training_numbers = int(training_percent*total_data_points)\n",
    "\n",
    "training_features = np.array([x1[:training_numbers],x2[:training_numbers]]).T\n",
    "training_labels = np.array(y[:training_numbers])\n",
    "\n",
    "testing_attributes=np.array([x1[training_numbers:],x2[training_numbers:]]).T\n",
    "testing_labels=np.array(y[training_numbers:])\n",
    "\n",
    "print(training_features.shape)\n",
    "print(testing_attributes.shape)\n",
    "print(training_labels.shape)\n",
    "print(testing_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb1efb",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dbaf349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0  weights  [[0.26750026 0.63508622 0.10280339 0.71914078 0.07685093]\n",
      " [0.13357125 0.39869683 0.773234   0.02418659 0.84258774]]  bias  [ 7.06752013e-06 -4.53275698e-06 -1.09830786e-06  5.02445408e-07\n",
      " -1.93890070e-06] loss 1.6596645713512694\n",
      "Iteration  1000  weights  [[0.27469056 0.62908383 0.10391438 0.7172349  0.07645791]\n",
      " [0.14186003 0.39482634 0.77021967 0.02737647 0.8379939 ]]  bias  [ 0.00691588 -0.00439258 -0.00109637  0.00046613 -0.00189306] loss 1.656776033919987\n",
      "Iteration  2000  weights  [[0.28154072 0.62341837 0.10501926 0.715257   0.07614623]\n",
      " [0.14982513 0.39123664 0.76721891 0.03047594 0.83351979]]  bias  [ 0.01351248 -0.00850462 -0.00218219  0.00086399 -0.00368967] loss 1.654133362884072\n",
      "Iteration  3000  weights  [[0.28805866 0.61806747 0.10612319 0.71321743 0.07591483]\n",
      " [0.15747336 0.38790855 0.76423761 0.03349321 0.82916369]]  bias  [ 0.01980454 -0.01235918 -0.00325409  0.00120219 -0.00539346] loss 1.6517157301319945\n",
      "Iteration  4000  weights  [[0.29425278 0.61301053 0.1072305  0.71112551 0.07576227]\n",
      " [0.16481204 0.38482441 0.76128073 0.03643575 0.82492349]]  bias  [ 0.02580007 -0.01597334 -0.00430831  0.00148803 -0.00700644] loss 1.6495039804071856\n",
      "Iteration  5000  weights  [[0.30013187 0.60822849 0.10834482 0.70898962 0.07568677]\n",
      " [0.17184891 0.38196793 0.75835247 0.03931036 0.82079675]]  bias  [ 0.03150742 -0.01936287 -0.00534176  0.00172807 -0.00853086] loss 1.6474804714026259\n",
      "Iteration  6000  weights  [[0.3057051  0.60370373 0.10946915 0.70681727 0.07568633]\n",
      " [0.17859208 0.37932405 0.75545629 0.04212321 0.8167808 ]]  bias  [ 0.0369352  -0.02254237 -0.0063519   0.00192823 -0.00996915] loss 1.645628940743415\n",
      "Iteration  7000  weights  [[0.31098189 0.59941989 0.11060592 0.70461516 0.07575872]\n",
      " [0.18504995 0.37687884 0.75259502 0.04487986 0.81287276]]  bias  [ 0.04209225 -0.0255254  -0.00733671  0.00209378 -0.01132392] loss 1.643934393587822\n",
      "Iteration  8000  weights  [[0.3159719  0.59536179 0.11175706 0.70238928 0.07590156]\n",
      " [0.19123121 0.37461936 0.74977092 0.04758535 0.80906959]]  bias  [ 0.04698754 -0.02832451 -0.00829462  0.00222944 -0.01259786] loss 1.6423830059667945\n",
      "Iteration  9000  weights  [[0.32068494 0.59151532 0.11292408 0.70014492 0.07611233]\n",
      " [0.1971447  0.3725336  0.74698578 0.05024419 0.80536814]]  bias  [ 0.05163017 -0.0309514  -0.00922444  0.00233943 -0.01379375] loss 1.6409620401043366\n",
      "Iteration  10000  weights  [[0.32513093 0.58786732 0.11410809 0.69788678 0.07638845]\n",
      " [0.20279943 0.37061039 0.74424093 0.05286047 0.80176521]]  bias  [ 0.05602929 -0.03341695 -0.01012535  0.00242746 -0.01491444] loss 1.6396597688626067\n",
      "Iteration  11000  weights  [[0.32931987 0.58440554 0.11530988 0.69561902 0.07672727]\n",
      " [0.20820448 0.36883932 0.7415373  0.05543781 0.7982575 ]]  bias  [ 0.06019408 -0.03573132 -0.0109968   0.00249684 -0.01596279] loss 1.638465407173807\n",
      "Iteration  12000  weights  [[0.33326174 0.58111855 0.11652993 0.69334527 0.0771261 ]\n",
      " [0.21336899 0.36721068 0.73887553 0.05797948 0.79484174]]  bias  [ 0.06413369 -0.03790398 -0.01183853  0.00255049 -0.01694167] loss 1.637369048887312\n",
      "Iteration  13000  weights  [[0.33696649 0.57799564 0.11776849 0.69106873 0.07758223]\n",
      " [0.21830208 0.3657154  0.73625594 0.06048838 0.79151462]]  bias  [ 0.06785722 -0.03994377 -0.01265048  0.00259097 -0.01785394] loss 1.6363616079016154\n",
      "Iteration  14000  weights  [[0.34044399 0.57502684 0.11902558 0.68879219 0.07809298]\n",
      " [0.22301283 0.36434501 0.7336786  0.06296711 0.78827287]]  bias  [ 0.07137365 -0.04185897 -0.01343279  0.00262054 -0.01870243] loss 1.6354347627889765\n",
      "Iteration  15000  weights  [[0.343704   0.57220281 0.12030103 0.68651807 0.07865567]\n",
      " [0.22751025 0.36309159 0.73114338 0.06541796 0.78511324]]  bias  [ 0.07469185 -0.0436573  -0.01418577  0.00264115 -0.01948993] loss 1.6345809043753163\n",
      "Iteration  16000  weights  [[0.34675613 0.56951479 0.12159454 0.68424847 0.07926765]\n",
      " [0.23180321 0.36194773 0.72864995 0.06784297 0.78203256]]  bias  [ 0.07782054 -0.04534601 -0.01490986  0.00265453 -0.02021919] loss 1.6337930859251941\n",
      "Iteration  17000  weights  [[0.34960981 0.56695458 0.12290565 0.68198519 0.07992634]\n",
      " [0.23590047 0.36090648 0.72619784 0.07024395 0.77902768]]  bias  [ 0.08076825 -0.04693189 -0.01560561  0.00266215 -0.02089289] loss 1.633064975715472\n",
      "Iteration  18000  weights  [[0.35227425 0.56451452 0.12423382 0.67972979 0.08062919]\n",
      " [0.23981059 0.35996134 0.72378645 0.07262249 0.77609555]]  bias  [ 0.08354331 -0.04842129 -0.01627367  0.0026653  -0.02151366] loss 1.6323908118730859\n",
      "Iteration  19000  weights  [[0.35475847 0.56218741 0.12557839 0.67748359 0.08137372]\n",
      " [0.24354195 0.35910622 0.72141508 0.07497999 0.77323319]]  bias  [ 0.08615387 -0.04982015 -0.01691477  0.00266509 -0.02208403] loss 1.6317653594119557\n",
      "Iteration  20000  weights  [[0.35707121 0.5599665  0.12693866 0.6752477  0.08215751]\n",
      " [0.24710274 0.35833538 0.71908293 0.07731769 0.77043768]]  bias  [ 0.08860779 -0.05113408 -0.01752968  0.00266245 -0.02260648] loss 1.6311838694394678\n",
      "Iteration  21000  weights  [[0.35922098 0.55784545 0.12831387 0.67302305 0.08297822]\n",
      " [0.2505009  0.35764348 0.71678914 0.07963667 0.76770623]]  bias  [ 0.09091275 -0.05236831 -0.01811923  0.0026582  -0.0230834 ] loss 1.6306420405206206\n",
      "Iteration  22000  weights  [[0.36121601 0.55581834 0.12970321 0.67081043 0.08383359]\n",
      " [0.25374414 0.35702549 0.71453281 0.08193788 0.7650361 ]]  bias  [ 0.09307614 -0.05352776 -0.01868428  0.002653   -0.0235171 ] loss 1.6301359821931116\n",
      "Iteration  23000  weights  [[0.36306426 0.55387958 0.13110584 0.66861047 0.08472143]\n",
      " [0.25683993 0.35647667 0.71231299 0.08422215 0.76242468]]  bias  [ 0.0951051  -0.05461703 -0.01922571  0.00264744 -0.0239098 ] loss 1.6296621806234386\n",
      "Iteration  24000  weights  [[0.36477339 0.55202394 0.13252091 0.6664237  0.08563964]\n",
      " [0.25979547 0.3559926  0.7101287  0.08649021 0.75986944]]  bias  [ 0.09700652 -0.05564045 -0.01974442  0.00264199 -0.02426365] loss 1.6292174663856667\n",
      "Iteration  25000  weights  [[0.36635079 0.55024653 0.13394755 0.66425052 0.08658619]\n",
      " [0.26261772 0.35556913 0.70797896 0.08874268 0.75736793]]  bias  [ 0.09878704 -0.05660208 -0.0202413   0.00263703 -0.02458068] loss 1.6287989843332735\n",
      "Iteration  26000  weights  [[0.36780356 0.54854273 0.13538489 0.66209127 0.08755912]\n",
      " [0.26531337 0.35520235 0.70586275 0.09098012 0.75491783]]  bias  [ 0.100453   -0.05750574 -0.02071727  0.00263289 -0.02486289] loss 1.62840416552219\n",
      "Iteration  27000  weights  [[0.3691385  0.54690822 0.13683209 0.65994617 0.0885566 ]\n",
      " [0.26788885 0.35488859 0.70377909 0.09320301 0.75251689]]  bias  [ 0.10201051 -0.058355   -0.02117319  0.00262982 -0.02511214] loss 1.6280307011310038\n",
      "Iteration  28000  weights  [[0.37036214 0.54533894 0.13828827 0.65781541 0.08957682]\n",
      " [0.27035032 0.35462441 0.70172697 0.09541175 0.75016296]]  bias  [ 0.10346541 -0.05915322 -0.02160997  0.00262803 -0.02533025] loss 1.6276765183131676\n",
      "Iteration  29000  weights  [[0.37148071 0.54383107 0.1397526  0.6556991  0.0906181 ]\n",
      " [0.27270369 0.3544066  0.69970543 0.0976067  0.74785401]]  bias  [ 0.10482327 -0.05990355 -0.02202846  0.00262767 -0.02551893] loss 1.6273397579063986\n",
      "Iteration  30000  weights  [[0.37250018 0.54238104 0.14122425 0.65359731 0.0916788 ]\n",
      " [0.27495461 0.35423211 0.69771348 0.09978817 0.74558805]]  bias  [ 0.10608944 -0.06060894 -0.02242952  0.00262884 -0.02567983] loss 1.6270187539166114\n",
      "Iteration  31000  weights  [[0.37342621 0.54098549 0.14270243 0.65151006 0.09275739]\n",
      " [0.27710847 0.3540981  0.69575019 0.10195643 0.74336323]]  bias  [ 0.107269   -0.06127219 -0.02281395  0.00263164 -0.0258145 ] loss 1.6267120146877088\n",
      "Iteration  32000  weights  [[0.37426424 0.53964126 0.14418635 0.64943734 0.09385239]\n",
      " [0.27917043 0.35400191 0.69381462 0.10411171 0.74117776]]  bias  [ 0.10836679 -0.06189588 -0.02318258  0.00263611 -0.02592445] loss 1.6264182056644185\n",
      "Iteration  33000  weights  [[0.37501942 0.53834541 0.14567525 0.64737911 0.0949624 ]\n",
      " [0.2811454  0.35394102 0.69190588 0.1062542  0.73902993]]  bias  [ 0.10938742 -0.06248247 -0.02353616  0.00264228 -0.02601107] loss 1.6261361336528963\n",
      "Iteration  34000  weights  [[0.37569665 0.53709514 0.14716841 0.64533529 0.09608609]\n",
      " [0.28303804 0.35391308 0.69002308 0.10838408 0.73691814]]  bias  [ 0.11033528 -0.06303424 -0.02387546  0.00265015 -0.02607573] loss 1.6258647324829547\n",
      "Iteration  35000  weights  [[0.37630059 0.53588785 0.14866513 0.6433058  0.09722221]\n",
      " [0.28485281 0.35391589 0.68816539 0.11050149 0.73484084]]  bias  [ 0.11121454 -0.06355336 -0.0242012   0.00265972 -0.02611969] loss 1.625603049976216\n",
      "Iteration  36000  weights  [[0.37683566 0.5347211  0.15016473 0.64129053 0.09836957]\n",
      " [0.28659393 0.35394737 0.68633199 0.11260656 0.73279657]]  bias  [ 0.11202914 -0.06404185 -0.02451407  0.00267094 -0.02614417] loss 1.6253502361261272\n",
      "Iteration  37000  weights  [[0.37730605 0.53359258 0.15166656 0.63928937 0.09952702]\n",
      " [0.28826541 0.35400559 0.68452208 0.11469939 0.73078395]]  bias  [ 0.11278284 -0.06450159 -0.02481474  0.0026838  -0.02615031] loss 1.625105532398314\n",
      "Iteration  38000  weights  [[0.37771575 0.53250014 0.15317    0.63730218 0.10069351]\n",
      " [0.28987106 0.35408872 0.6827349  0.11678009 0.72880165]]  bias  [ 0.1134792  -0.06493438 -0.02510385  0.00269824 -0.0261392 ] loss 1.6248682620630643\n",
      "Iteration  39000  weights  [[0.3780685  0.53144175 0.15467447 0.63532883 0.10186803]\n",
      " [0.29141449 0.35419504 0.68096972 0.11884874 0.72684843]]  bias  [ 0.11412157 -0.06534187 -0.02538202  0.0027142  -0.02611188] loss 1.6246378214756096\n",
      "Iteration  40000  weights  [[0.37836788 0.53041551 0.1561794  0.63336917 0.10304961]\n",
      " [0.29289913 0.35432296 0.67922584 0.1209054  0.7249231 ]]  bias  [ 0.11471315 -0.06572563 -0.02564983  0.00273163 -0.02606931] loss 1.6244136722241365\n",
      "Iteration  41000  weights  [[0.37861725 0.52941965 0.15768426 0.63142306 0.10423736]\n",
      " [0.2943282  0.35447097 0.67750258 0.12295014 0.72302453]]  bias  [ 0.11525694 -0.06608714 -0.02590784  0.00275047 -0.02601242] loss 1.6241953340700441\n",
      "Iteration  42000  weights  [[0.37881978 0.52845248 0.15918854 0.62949034 0.10543043]\n",
      " [0.29570478 0.35463767 0.67579931 0.12498301 0.72115166]]  bias  [ 0.1157558  -0.06642776 -0.02615659  0.00277064 -0.02594208] loss 1.6239823786096284\n",
      "Iteration  43000  weights  [[0.3789785  0.52751245 0.16069176 0.62757086 0.10662802]\n",
      " [0.29703176 0.35482172 0.67411539 0.12700407 0.71930348]]  bias  [ 0.11621241 -0.0667488  -0.02639658  0.00279208 -0.02585912] loss 1.6237744235911598\n",
      "Iteration  44000  weights  [[0.37909622 0.52659806 0.16219346 0.62566447 0.10782937]\n",
      " [0.29831189 0.35502188 0.67245025 0.12901337 0.71747903]]  bias  [ 0.11662932 -0.06705145 -0.02662829  0.00281472 -0.0257643 ] loss 1.6235711278260117\n",
      "Iteration  45000  weights  [[0.37917564 0.52570794 0.16369321 0.62377102 0.10903377]\n",
      " [0.29954775 0.35523701 0.67080331 0.13101094 0.71567742]]  bias  [ 0.11700893 -0.06733687 -0.02685219  0.00283849 -0.02565836] loss 1.6233721866371542\n",
      "Iteration  46000  weights  [[0.37921927 0.5248408  0.16519061 0.62189035 0.11024056]\n",
      " [0.30074179 0.35546599 0.66917404 0.13299682 0.71389777]]  bias  [ 0.1173535  -0.06760611 -0.02706871  0.00286332 -0.02554201] loss 1.6231773277927601\n",
      "Iteration  47000  weights  [[0.37922949 0.52399541 0.16668526 0.62002232 0.1114491 ]\n",
      " [0.30189632 0.35570782 0.66756193 0.13497105 0.7121393 ]]  bias  [ 0.11766517 -0.06786018 -0.02727826  0.00288914 -0.02541588] loss 1.6229863078770126\n",
      "Iteration  48000  weights  [[0.37920855 0.52317062 0.16817682 0.61816677 0.11265882]\n",
      " [0.30301352 0.35596153 0.66596648 0.13693367 0.71040122]]  bias  [ 0.11794594 -0.0681     -0.02748124  0.00291589 -0.02528059] loss 1.6227989090542425\n",
      "Iteration  49000  weights  [[0.37915855 0.52236538 0.16966494 0.61632357 0.11386915]\n",
      " [0.30409544 0.35622624 0.66438723 0.1388847  0.70868281]]  bias  [ 0.11819772 -0.06832647 -0.02767801  0.0029435  -0.02513674] loss 1.6226149361863986\n",
      "Iteration  50000  weights  [[0.37908149 0.52157866 0.1711493  0.61449257 0.11507957]\n",
      " [0.30514402 0.35650108 0.66282374 0.14082419 0.70698339]]  bias  [ 0.11842229 -0.0685404  -0.02786894  0.0029719  -0.02498485] loss 1.622434214267446\n",
      "Iteration  51000  weights  [[0.37897923 0.52080953 0.1726296  0.61267363 0.1162896 ]\n",
      " [0.30616109 0.35678529 0.66127557 0.14275216 0.70530231]]  bias  [ 0.11862133 -0.06874256 -0.02805435  0.00300104 -0.02482546] loss 1.6222565861416776\n",
      "Iteration  52000  weights  [[0.37885353 0.52005709 0.17410556 0.61086662 0.11749878]\n",
      " [0.30714837 0.35707812 0.65974233 0.14466865 0.70363895]]  bias  [ 0.11879643 -0.06893369 -0.02823456  0.00303085 -0.02465904] loss 1.622081910476022\n",
      "Iteration  53000  weights  [[0.37870605 0.51932051 0.17557692 0.6090714  0.11870669]\n",
      " [0.30810748 0.35737888 0.65822363 0.14657369 0.70199273]]  bias  [ 0.11894907 -0.06911445 -0.02840986  0.00306129 -0.02448605] loss 1.6219100599593244\n",
      "Iteration  54000  weights  [[0.37853835 0.51859902 0.17704344 0.60728786 0.11991292]\n",
      " [0.30903996 0.35768693 0.65671911 0.14846731 0.70036311]]  bias  [ 0.11908067 -0.06928549 -0.02858054  0.00309228 -0.02430692] loss 1.6217409197042436\n",
      "Iteration  55000  weights  [[0.37835188 0.51789187 0.17850488 0.60551585 0.1211171 ]\n",
      " [0.30994724 0.35800165 0.65522841 0.15034955 0.69874956]]  bias  [ 0.11919254 -0.06944739 -0.02874687  0.00312378 -0.02412206] loss 1.6215743858298184\n",
      "Iteration  56000  weights  [[0.37814802 0.51719837 0.17996103 0.60375527 0.12231888]\n",
      " [0.31083068 0.35832248 0.65375121 0.15222045 0.6971516 ]]  bias  [ 0.11928592 -0.06960073 -0.02890909  0.00315574 -0.02393185] loss 1.6214103642049933\n",
      "Iteration  57000  weights  [[0.37792806 0.51651789 0.1814117  0.60200599 0.12351794]\n",
      " [0.31169155 0.35864889 0.65228719 0.15408003 0.69556876]]  bias  [ 0.11936199 -0.06974603 -0.02906743  0.00318812 -0.02373665] loss 1.6212487693354118\n",
      "Iteration  58000  weights  [[0.37769321 0.51584981 0.1828567  0.6002679  0.12471396]\n",
      " [0.31253106 0.35898038 0.65083605 0.15592833 0.6940006 ]]  bias  [ 0.11942182 -0.06988377 -0.02922213  0.00322087 -0.02353679] loss 1.6210895233776257\n",
      "Iteration  59000  weights  [[0.37744461 0.51519356 0.18429585 0.59854089 0.12590667]\n",
      " [0.31335034 0.35931648 0.6493975  0.1577654  0.6924467 ]]  bias  [ 0.11946647 -0.07001441 -0.0293734   0.00325393 -0.0233326 ] loss 1.6209325552665335\n",
      "Iteration  60000  weights  [[0.37718332 0.5145486  0.18572901 0.59682485 0.12709581]\n",
      " [0.31415045 0.35965677 0.64797127 0.15959126 0.69090667]]  bias  [ 0.1194969  -0.0701384  -0.02952142  0.00328729 -0.02312436] loss 1.6207777999433757\n",
      "Iteration  61000  weights  [[0.37691035 0.51391443 0.18715602 0.59511967 0.12828111]\n",
      " [0.31493238 0.36000082 0.64655711 0.16140596 0.68938014]]  bias  [ 0.11951401 -0.07025613 -0.0296664   0.00332089 -0.02291237] loss 1.620625197672951\n",
      "Iteration  62000  weights  [[0.37662664 0.51329058 0.18857674 0.59342525 0.12946236]\n",
      " [0.3156971  0.36034826 0.64515476 0.16320954 0.68786676]]  bias  [ 0.11951867 -0.07036798 -0.0298085   0.0033547  -0.02269688] loss 1.6204746934399779\n",
      "Iteration  63000  weights  [[0.37633307 0.5126766  0.18999107 0.59174151 0.13063934]\n",
      " [0.31644547 0.36069874 0.643764   0.16500204 0.68636618]]  bias  [ 0.11951167 -0.07047431 -0.02994789  0.00338868 -0.02247815] loss 1.620326236415591\n",
      "Iteration  64000  weights  [[0.37603045 0.51207207 0.19139887 0.59006833 0.13181186]\n",
      " [0.31717833 0.36105191 0.64238458 0.1667835  0.6848781 ]]  bias  [ 0.11949377 -0.07057545 -0.03008473  0.00342281 -0.02225641] loss 1.620179779485975\n",
      "Iteration  65000  weights  [[0.37571958 0.5114766  0.19280004 0.58840562 0.13297973]\n",
      " [0.31789647 0.36140747 0.64101631 0.16855396 0.68340221]]  bias  [ 0.11946569 -0.07067172 -0.03021916  0.00345705 -0.02203187] loss 1.6200352788360082\n",
      "Iteration  66000  weights  [[0.37540117 0.51088983 0.1941945  0.5867533  0.13414279]\n",
      " [0.31860062 0.36176513 0.63965898 0.17031347 0.68193823]]  bias  [ 0.11942808 -0.0707634  -0.03035132  0.00349138 -0.02180474] loss 1.619892693581591\n",
      "Iteration  67000  weights  [[0.3750759  0.51031139 0.19558214 0.58511128 0.13530086]\n",
      " [0.31929148 0.3621246  0.63831239 0.17206207 0.68048588]]  bias  [ 0.11938157 -0.07085078 -0.03048134  0.00352576 -0.02157521] loss 1.6197519854450382\n",
      "Iteration  68000  weights  [[0.3747444  0.50974098 0.1969629  0.58347947 0.13645382]\n",
      " [0.3199697  0.36248564 0.63697636 0.1737998  0.67904492]]  bias  [ 0.11932675 -0.0709341  -0.03060935  0.00356018 -0.02134347] loss 1.619613118468547\n",
      "Iteration  69000  weights  [[0.37440729 0.50917827 0.1983367  0.58185779 0.13760153]\n",
      " [0.32063589 0.36284801 0.63565071 0.17552672 0.6776151 ]]  bias  [ 0.11926416 -0.07101362 -0.03073546  0.00359461 -0.02110969] loss 1.619476058761315\n",
      "Iteration  70000  weights  [[0.3740651  0.50862298 0.19970348 0.58024616 0.13874386]\n",
      " [0.32129062 0.36321148 0.63433527 0.17724286 0.67619619]]  bias  [ 0.11919433 -0.07108955 -0.03085978  0.00362903 -0.02087403] loss 1.6193407742763941\n",
      "Iteration  71000  weights  [[0.37371837 0.50807483 0.20106317 0.5786445  0.13988071]\n",
      " [0.32193445 0.36357585 0.63302988 0.17894828 0.67478797]]  bias  [ 0.11911773 -0.07116211 -0.03098241  0.00366342 -0.02063663] loss 1.6192072346138007\n",
      "Iteration  72000  weights  [[0.37336758 0.50753358 0.20241573 0.57705272 0.14101196]\n",
      " [0.32256788 0.36394092 0.63173438 0.18064302 0.67339023]]  bias  [ 0.11903482 -0.0712315  -0.03110345  0.00369776 -0.02039764] loss 1.6190754108468082\n",
      "Iteration  73000  weights  [[0.3730132  0.50699897 0.20376111 0.57547077 0.14213753]\n",
      " [0.32319138 0.36430651 0.63044862 0.18232714 0.67200277]]  bias  [ 0.11894604 -0.0712979  -0.03122297  0.00373203 -0.02015719] loss 1.6189452753687072\n",
      "Iteration  74000  weights  [[0.37265564 0.50647079 0.20509928 0.57389855 0.14325732]\n",
      " [0.32380542 0.36467245 0.62917247 0.18400067 0.67062541]]  bias  [ 0.11885177 -0.07136149 -0.03134108  0.00376622 -0.01991541] loss 1.618816801757616\n",
      "Iteration  75000  weights  [[0.37229531 0.50594881 0.20643019 0.57233601 0.14437127]\n",
      " [0.32441041 0.36503858 0.62790579 0.18566368 0.66925796]]  bias  [ 0.11875239 -0.07142244 -0.03145785  0.00380031 -0.01967241] loss 1.6186899646572228\n",
      "Iteration  76000  weights  [[0.37193257 0.50543284 0.20775382 0.57078307 0.14547929]\n",
      " [0.32500675 0.36540476 0.62664844 0.18731621 0.66790026]]  bias  [ 0.11864825 -0.07148089 -0.03157336  0.00383429 -0.0194283 ] loss 1.618564739671571\n",
      "Iteration  77000  weights  [[0.37156779 0.50492267 0.20907013 0.56923966 0.14658132]\n",
      " [0.32559482 0.36577084 0.6254003  0.18895831 0.66655214]]  bias  [ 0.1185397  -0.07153699 -0.03168766  0.00386814 -0.01918318] loss 1.6184411032722354\n",
      "Iteration  78000  weights  [[0.37120128 0.50441815 0.21037912 0.56770571 0.14767731]\n",
      " [0.32617497 0.36613671 0.62416126 0.19059004 0.66521345]]  bias  [ 0.11842702 -0.07159087 -0.03180084  0.00390185 -0.01893715] loss 1.6183190327164088\n",
      "Iteration  79000  weights  [[0.37083336 0.50391908 0.21168077 0.56618117 0.14876721]\n",
      " [0.32674753 0.36650223 0.62293118 0.19221144 0.66388403]]  bias  [ 0.11831052 -0.07164267 -0.03191295  0.00393541 -0.01869031] loss 1.618198505974617\n",
      "Iteration  80000  weights  [[0.37046429 0.50342532 0.21297505 0.56466595 0.14985096]\n",
      " [0.32731282 0.36686731 0.62170998 0.19382257 0.66256375]]  bias  [ 0.11819046 -0.0716925  -0.03202405  0.00396882 -0.01844273] loss 1.6180795016669063\n",
      "Iteration  81000  weights  [[0.37009436 0.50293671 0.21426197 0.56316002 0.15092853]\n",
      " [0.32787113 0.36723182 0.62049752 0.19542348 0.66125247]]  bias  [ 0.11806711 -0.07174047 -0.0321342   0.00400205 -0.0181945 ] loss 1.6179619990065095\n",
      "Iteration  82000  weights  [[0.3697238  0.50245311 0.21554151 0.56166329 0.15199988]\n",
      " [0.32842273 0.36759569 0.61929372 0.19701422 0.65995006]]  bias  [ 0.11794069 -0.07178669 -0.03224344  0.00403511 -0.01794568] loss 1.6178459777500778\n",
      "Iteration  83000  weights  [[0.36935284 0.50197438 0.21681367 0.5601757  0.15306498]\n",
      " [0.32896789 0.36795881 0.61809848 0.19859485 0.6586564 ]]  bias  [ 0.11781144 -0.07183125 -0.03235182  0.00406799 -0.01769636] loss 1.6177314181537217\n",
      "Iteration  84000  weights  [[0.36898171 0.5015004  0.21807846 0.55869721 0.15412381]\n",
      " [0.32950685 0.3683211  0.61691169 0.20016541 0.65737137]]  bias  [ 0.11767956 -0.07187425 -0.03245939  0.00410067 -0.01744659] loss 1.6176183009341456\n",
      "Iteration  85000  weights  [[0.36861059 0.50103104 0.21933587 0.55722775 0.15517633]\n",
      " [0.33003984 0.3686825  0.61573326 0.20172597 0.65609486]]  bias  [ 0.11754525 -0.07191577 -0.03256619  0.00413315 -0.01719644] loss 1.6175066072342756\n",
      "Iteration  86000  weights  [[0.36823969 0.50056619 0.22058591 0.55576726 0.15622253]\n",
      " [0.33056708 0.36904291 0.61456311 0.20327657 0.65482675]]  bias  [ 0.1174087  -0.07195589 -0.03267227  0.00416543 -0.01694597] loss 1.6173963185928475\n",
      "Iteration  87000  weights  [[0.36786916 0.50010574 0.22182859 0.55431568 0.15726241]\n",
      " [0.33108877 0.36940229 0.61340115 0.20481727 0.65356694]]  bias  [ 0.11727007 -0.0719947  -0.03277765  0.0041975  -0.01669523] loss 1.617287416917471\n",
      "Iteration  88000  weights  [[0.36749917 0.49964959 0.22306392 0.55287296 0.15829593]\n",
      " [0.33160511 0.36976056 0.6122473  0.20634812 0.65231534]]  bias  [ 0.11712954 -0.07203225 -0.03288237  0.00422935 -0.01644426] loss 1.6171798844607606\n",
      "Iteration  89000  weights  [[0.36712988 0.49919764 0.22429191 0.55143905 0.1593231 ]\n",
      " [0.33211628 0.37011766 0.61110147 0.20786917 0.65107183]]  bias  [ 0.11698724 -0.07206863 -0.03298647  0.00426099 -0.01619312] loss 1.6170737037991632\n",
      "Iteration  90000  weights  [[0.36676141 0.4987498  0.22551258 0.55001388 0.16034391]\n",
      " [0.33262245 0.37047356 0.60996358 0.20938049 0.64983634]]  bias  [ 0.11684332 -0.07210388 -0.03308998  0.0042924  -0.01594186] loss 1.6169688578141592\n",
      "Iteration  91000  weights  [[0.3663939  0.49830598 0.22672593 0.54859741 0.16135836]\n",
      " [0.33312377 0.37082818 0.60883356 0.21088212 0.64860878]]  bias  [ 0.11669792 -0.07213808 -0.03319292  0.00432359 -0.01569051] loss 1.6168653296755513\n",
      "Iteration  92000  weights  [[0.36602747 0.49786609 0.227932   0.54718958 0.16236644]\n",
      " [0.33362041 0.3711815  0.60771134 0.21237412 0.64738904]]  bias  [ 0.11655116 -0.07217127 -0.03329533  0.00435454 -0.01543911] loss 1.6167631028265899\n",
      "Iteration  93000  weights  [[0.36566223 0.49743006 0.22913079 0.54579033 0.16336816]\n",
      " [0.3341125  0.37153347 0.60659684 0.21385655 0.64617706]]  bias  [ 0.11640316 -0.07220351 -0.03339722  0.00438527 -0.0151877 ] loss 1.616662160970718\n",
      "Iteration  94000  weights  [[0.36529828 0.49699781 0.23032234 0.54439962 0.16436353]\n",
      " [0.33460019 0.37188404 0.60548999 0.21532944 0.64497275]]  bias  [ 0.11625402 -0.07223484 -0.03349862  0.00441575 -0.01493632] loss 1.6165624880597365\n",
      "Iteration  95000  weights  [[0.36493572 0.49656927 0.23150665 0.5430174  0.16535254]\n",
      " [0.33508359 0.37223319 0.60439073 0.21679287 0.64377604]]  bias  [ 0.11610385 -0.07226532 -0.03359955  0.00444601 -0.01468499] loss 1.6164640682832232\n",
      "Iteration  96000  weights  [[0.36457463 0.49614437 0.23268376 0.54164361 0.16633521]\n",
      " [0.33556283 0.37258087 0.60329898 0.21824689 0.64258684]]  bias  [ 0.11595274 -0.07229498 -0.03370004  0.00447602 -0.01443374] loss 1.6163668860590483\n",
      "Iteration  97000  weights  [[0.3642151  0.49572305 0.23385368 0.54027821 0.16731155]\n",
      " [0.33603803 0.37292707 0.60221469 0.21969154 0.64140509]]  bias  [ 0.11580079 -0.07232386 -0.03380011  0.00450579 -0.01418261] loss 1.6162709260248658\n",
      "Iteration  98000  weights  [[0.36385719 0.49530523 0.23501645 0.53892114 0.16828156]\n",
      " [0.33650929 0.37327175 0.60113778 0.22112689 0.64023071]]  bias  [ 0.11564807 -0.07235201 -0.03389977  0.00453533 -0.01393162] loss 1.6161761730304502\n",
      "Iteration  99000  weights  [[0.36350099 0.49489087 0.2361721  0.53757235 0.16924527]\n",
      " [0.33697671 0.37361488 0.60006821 0.22255298 0.63906364]]  bias  [ 0.11549467 -0.07237946 -0.03399903  0.00456462 -0.0136808 ] loss 1.6160826121307899\n",
      "Iteration  100000  weights  [[0.36314654 0.49447991 0.23732064 0.5362318  0.17020269]\n",
      " [0.33744039 0.37395645 0.5990059  0.22396987 0.63790381]]  bias  [ 0.11534066 -0.07240624 -0.03409792  0.00459367 -0.01343016] loss 1.615990228579836\n",
      "Iteration  101000  weights  [[0.36279392 0.49407229 0.23846211 0.53489943 0.17115383]\n",
      " [0.33790042 0.37429644 0.5979508  0.22537762 0.63675115]]  bias  [ 0.1151861  -0.07243238 -0.03419646  0.00462247 -0.01317973] loss 1.6158990078248334\n",
      "Iteration  102000  weights  [[0.36244317 0.49366795 0.23959653 0.53357521 0.17209872]\n",
      " [0.33835688 0.37463481 0.59690285 0.22677628 0.63560561]]  bias  [ 0.11503106 -0.07245792 -0.03429464  0.00465103 -0.01292953] loss 1.6158089355011651\n",
      "Iteration  103000  weights  [[0.36209434 0.49326686 0.24072394 0.53225908 0.17303736]\n",
      " [0.33880986 0.37497156 0.59586199 0.22816589 0.63446712]]  bias  [ 0.11487561 -0.07248288 -0.0343925   0.00467935 -0.01267958] loss 1.6157199974276457\n",
      "Iteration  104000  weights  [[0.36174747 0.49286896 0.24184437 0.53095099 0.17396978]\n",
      " [0.33925944 0.37530668 0.59482817 0.22954652 0.63333561]]  bias  [ 0.11471979 -0.07250729 -0.03449003  0.00470743 -0.01242989] loss 1.6156321796022106\n",
      "Iteration  105000  weights  [[0.36140261 0.4924742  0.24295785 0.52965091 0.17489601]\n",
      " [0.33970568 0.37564014 0.59380134 0.23091822 0.63221105]]  bias  [ 0.11456367 -0.07253117 -0.03458726  0.00473526 -0.01218049] loss 1.6155454681979533\n",
      "Iteration  106000  weights  [[0.36105979 0.49208254 0.24406441 0.52835877 0.17581606]\n",
      " [0.34014866 0.37597193 0.59278143 0.23228104 0.63109335]]  bias  [ 0.11440728 -0.07255455 -0.03468419  0.00476285 -0.01193138] loss 1.615459849559477\n",
      "Iteration  107000  weights  [[0.36071905 0.49169395 0.24516409 0.52707454 0.17672996]\n",
      " [0.34058844 0.37630205 0.59176841 0.23363504 0.62998248]]  bias  [ 0.11425068 -0.07257745 -0.03478084  0.0047902  -0.01168259] loss 1.6153753101995085\n",
      "Iteration  108000  weights  [[0.36038041 0.49130837 0.24625691 0.52579817 0.17763772]\n",
      " [0.34102509 0.37663048 0.59076221 0.23498026 0.62887838]]  bias  [ 0.1140939  -0.07259988 -0.0348772   0.0048173  -0.01143412] loss 1.615291836795759\n",
      "Iteration  109000  weights  [[0.3600439  0.49092577 0.24734291 0.52452962 0.17853939]\n",
      " [0.34145866 0.37695721 0.58976279 0.23631676 0.62778099]]  bias  [ 0.11393699 -0.07262188 -0.0349733   0.00484417 -0.01118599] loss 1.6152094161879909\n",
      "Iteration  110000  weights  [[0.35970955 0.49054611 0.24842212 0.52326883 0.17943497]\n",
      " [0.34188922 0.37728224 0.5887701  0.2376446  0.62669027]]  bias  [ 0.11377999 -0.07264345 -0.03506914  0.00487079 -0.0109382 ] loss 1.6151280353752728\n",
      "Iteration  111000  weights  [[0.35937738 0.49016935 0.24949459 0.52201577 0.1803245 ]\n",
      " [0.3423168  0.37760557 0.58778408 0.23896382 0.62560616]]  bias  [ 0.11362293 -0.07266461 -0.03516472  0.00489718 -0.01069078] loss 1.6150476815134023\n",
      "Iteration  112000  weights  [[0.3590474  0.48979546 0.25056033 0.52077039 0.181208  ]\n",
      " [0.34274148 0.37792717 0.58680469 0.24027447 0.62452861]]  bias  [ 0.11346584 -0.07268538 -0.03526006  0.00492333 -0.01044372] loss 1.6149683419124685\n",
      "Iteration  113000  weights  [[0.35871963 0.48942441 0.25161939 0.51953265 0.18208549]\n",
      " [0.34316328 0.37824706 0.58583189 0.24157662 0.62345757]]  bias  [ 0.11330875 -0.07270578 -0.03535516  0.00494924 -0.01019705] loss 1.6148900040345462\n",
      "Iteration  114000  weights  [[0.35839409 0.48905616 0.25267181 0.51830251 0.18295702]\n",
      " [0.34358227 0.37856523 0.58486562 0.2428703  0.622393  ]]  bias  [ 0.11315169 -0.07272582 -0.03545003  0.00497492 -0.00995076] loss 1.6148126554915079\n",
      "Iteration  115000  weights  [[0.35807078 0.48869068 0.25371761 0.51707991 0.1838226 ]\n",
      " [0.34399848 0.37888167 0.58390584 0.24415558 0.62133485]]  bias  [ 0.11299469 -0.07274551 -0.03554468  0.00500036 -0.00970486] loss 1.6147362840429305\n",
      "Iteration  116000  weights  [[0.35774973 0.48832793 0.25475684 0.51586482 0.18468226]\n",
      " [0.34441196 0.37919638 0.5829525  0.2454325  0.62028308]]  bias  [ 0.11283777 -0.07276487 -0.0356391   0.00502557 -0.00945937] loss 1.614660877594097\n",
      "Iteration  117000  weights  [[0.35743093 0.4879679  0.25578952 0.51465719 0.18553604]\n",
      " [0.34482274 0.37950936 0.58200557 0.24670111 0.61923763]]  bias  [ 0.11268096 -0.07278391 -0.03573331  0.00505055 -0.00921429] loss 1.614586424194075\n",
      "Iteration  118000  weights  [[0.3571144  0.48761055 0.2568157  0.51345698 0.18638395]\n",
      " [0.34523087 0.37982062 0.58106499 0.24796148 0.61819847]]  bias  [ 0.11252427 -0.07280264 -0.0358273   0.00507531 -0.00896963] loss 1.6145129120338702\n",
      "Iteration  119000  weights  [[0.35680013 0.48725584 0.25783541 0.51226415 0.18722604]\n",
      " [0.34563638 0.38013014 0.58013072 0.24921363 0.61716554]]  bias  [ 0.11236773 -0.07282107 -0.0359211   0.00509983 -0.00872539] loss 1.6144403294446388\n",
      "Iteration  120000  weights  [[0.35648814 0.48690376 0.25884869 0.51107866 0.18806233]\n",
      " [0.34603931 0.38043794 0.57920271 0.25045764 0.61613882]]  bias  [ 0.11221135 -0.07283921 -0.03601469  0.00512412 -0.00848158] loss 1.6143686648959623\n",
      "Iteration  121000  weights  [[0.35617842 0.48655428 0.25985557 0.50990046 0.18889284]\n",
      " [0.34643969 0.380744   0.57828094 0.25169354 0.61511825]]  bias  [ 0.11205515 -0.07285707 -0.03610808  0.0051482  -0.0082382 ] loss 1.6142979069941683\n",
      "Iteration  122000  weights  [[0.35587098 0.48620737 0.26085609 0.50872952 0.18971762]\n",
      " [0.34683755 0.38104834 0.57736534 0.25292139 0.61410379]]  bias  [ 0.11189915 -0.07287466 -0.03620128  0.00517205 -0.00799526] loss 1.614228044480702\n",
      "Iteration  123000  weights  [[0.35556582 0.48586301 0.26185029 0.50756578 0.19053669]\n",
      " [0.34723293 0.38135096 0.57645589 0.25414124 0.61309541]]  bias  [ 0.11174336 -0.07289199 -0.03629429  0.00519567 -0.00775276] loss 1.6141590662305336\n",
      "Iteration  124000  weights  [[0.35526293 0.48552116 0.26283819 0.50640922 0.19135008]\n",
      " [0.34762585 0.38165185 0.57555254 0.25535313 0.61209305]]  bias  [ 0.1115878  -0.07290907 -0.03638711  0.00521908 -0.00751071] loss 1.6140909612506098\n",
      "Iteration  125000  weights  [[0.35496232 0.48518182 0.26381985 0.50525978 0.19215782]\n",
      " [0.34801634 0.38195103 0.57465525 0.25655712 0.61109669]]  bias  [ 0.11143248 -0.07292591 -0.03647974  0.00524227 -0.0072691 ] loss 1.6140237186783342\n",
      "Iteration  126000  weights  [[0.35466398 0.48484494 0.26479529 0.50411743 0.19295994]\n",
      " [0.34840443 0.38224849 0.57376397 0.25775325 0.61010628]]  bias  [ 0.11127741 -0.0729425  -0.0365722   0.00526525 -0.00702796] loss 1.6139573277800796\n",
      "Iteration  127000  weights  [[0.3543679  0.48451052 0.26576454 0.50298213 0.19375648]\n",
      " [0.34879014 0.38254423 0.57287868 0.25894158 0.60912178]]  bias  [ 0.1111226  -0.07295887 -0.03666448  0.00528801 -0.00678726] loss 1.613891777949732\n",
      "Iteration  128000  weights  [[0.35407409 0.48417853 0.26672766 0.50185384 0.19454746]\n",
      " [0.34917351 0.38283827 0.57199932 0.26012216 0.60814316]]  bias  [ 0.11096806 -0.07297501 -0.03675658  0.00531055 -0.00654703] loss 1.6138270587072547\n",
      "Iteration  129000  weights  [[0.35378254 0.48384894 0.26768467 0.50073251 0.19533292]\n",
      " [0.34955456 0.38313061 0.57112586 0.26129502 0.60717037]]  bias  [ 0.1108138  -0.07299094 -0.0368485   0.00533289 -0.00630726] loss 1.613763159697283\n",
      "Iteration  130000  weights  [[0.35349324 0.48352174 0.26863561 0.49961811 0.19611289]\n",
      " [0.3499333  0.38342124 0.57025827 0.26246023 0.60620338]]  bias  [ 0.11065984 -0.07300665 -0.03694026  0.00535502 -0.00606795] loss 1.6137000706877354\n",
      "Iteration  131000  weights  [[0.35320618 0.4831969  0.26958051 0.4985106  0.19688739]\n",
      " [0.35030977 0.38371018 0.5693965  0.26361782 0.60524215]]  bias  [ 0.11050617 -0.07302216 -0.03703184  0.00537694 -0.00582911] loss 1.6136377815684477\n",
      "Iteration  132000  weights  [[0.35292136 0.4828744  0.27051942 0.49740993 0.19765647]\n",
      " [0.35068399 0.38399743 0.56854051 0.26476785 0.60428664]]  bias  [ 0.1103528  -0.07303747 -0.03712326  0.00539866 -0.00559074] loss 1.6135762823498287\n",
      "Iteration  133000  weights  [[0.35263877 0.48255422 0.27145236 0.49631607 0.19842015]\n",
      " [0.35105597 0.384283   0.56769027 0.26591036 0.60333682]]  bias  [ 0.11019975 -0.07305259 -0.03721451  0.00542018 -0.00535283] loss 1.6135155631615297\n",
      "Iteration  134000  weights  [[0.3523584  0.48223635 0.27237938 0.49522899 0.19917846]\n",
      " [0.35142574 0.38456689 0.56684574 0.2670454  0.60239266]]  bias  [ 0.11004702 -0.07306751 -0.03730559  0.00544149 -0.0051154 ] loss 1.6134556142511318\n",
      "Iteration  135000  weights  [[0.35208025 0.48192076 0.27330051 0.49414863 0.19993143]\n",
      " [0.35179331 0.3848491  0.56600688 0.26817302 0.6014541 ]]  bias  [ 0.1098946  -0.07308225 -0.03739651  0.00546261 -0.00487845] loss 1.6133964259828537\n",
      "Iteration  136000  weights  [[0.3518043  0.48160743 0.27421578 0.49307496 0.20067911]\n",
      " [0.35215872 0.38512965 0.56517365 0.26929327 0.60052113]]  bias  [ 0.10974252 -0.07309681 -0.03748727  0.00548352 -0.00464196] loss 1.6133379888362687\n",
      "Iteration  137000  weights  [[0.35153054 0.48129635 0.27512524 0.49200794 0.20142151]\n",
      " [0.35252197 0.38540854 0.56434603 0.27040618 0.59959371]]  bias  [ 0.10959077 -0.0731112  -0.03757787  0.00550425 -0.00440595] loss 1.6132802934050399\n",
      "Iteration  138000  weights  [[0.35125896 0.4809875  0.27602891 0.49094754 0.20215867]\n",
      " [0.35288309 0.38568577 0.56352396 0.27151181 0.59867179]]  bias  [ 0.10943936 -0.07312541 -0.03766831  0.00552477 -0.00417042] loss 1.613223330395668\n",
      "Iteration  139000  weights  [[0.35098956 0.48068085 0.27692683 0.48989372 0.20289061]\n",
      " [0.35324209 0.38596136 0.56270742 0.27261021 0.59775535]]  bias  [ 0.1092883  -0.07313945 -0.03775859  0.00554511 -0.00393537] loss 1.6131670906262545\n",
      "Iteration  140000  weights  [[0.35072233 0.4803764  0.27781904 0.48884643 0.20361738]\n",
      " [0.35359899 0.3862353  0.56189638 0.27370142 0.59684435]]  bias  [ 0.10913757 -0.07315333 -0.03784871  0.00556526 -0.00370079] loss 1.6131115650252736\n",
      "Iteration  141000  weights  [[0.35045724 0.48007411 0.27870557 0.48780565 0.20433901]\n",
      " [0.35395381 0.38650761 0.56109078 0.27478548 0.59593875]]  bias  [ 0.1089872  -0.07316705 -0.03793868  0.00558522 -0.00346669] loss 1.6130567446303623\n",
      "Iteration  142000  weights  [[0.3501943  0.47977398 0.27958646 0.48677132 0.20505552]\n",
      " [0.35430656 0.38677828 0.56029061 0.27586244 0.59503853]]  bias  [ 0.10883718 -0.07318061 -0.03802849  0.005605   -0.00323307] loss 1.6130026205871169\n",
      "Iteration  143000  weights  [[0.34993348 0.47947599 0.28046174 0.48574343 0.20576694]\n",
      " [0.35465726 0.38704734 0.55949582 0.27693234 0.59414365]]  bias  [ 0.10868751 -0.07319402 -0.03811815  0.00562459 -0.00299993] loss 1.6129491841479064\n",
      "Iteration  144000  weights  [[0.34967479 0.47918012 0.28133145 0.48472192 0.20647331]\n",
      " [0.35500593 0.38731479 0.55870638 0.27799524 0.59325408]]  bias  [ 0.1085382  -0.07320728 -0.03820765  0.005644   -0.00276727] loss 1.612896426670694\n",
      "Iteration  145000  weights  [[0.3494182  0.47888635 0.28219562 0.48370676 0.20717465]\n",
      " [0.35535259 0.38758062 0.55792226 0.27905117 0.59236979]]  bias  [ 0.10838925 -0.07322039 -0.038297    0.00566322 -0.00253508] loss 1.612844339617871\n",
      "Iteration  146000  weights  [[0.3491637  0.47859467 0.28305429 0.48269791 0.207871  ]\n",
      " [0.35569724 0.38784486 0.55714342 0.28010018 0.59149073]]  bias  [ 0.10824066 -0.07323336 -0.03838619  0.00568228 -0.00230338] loss 1.6127929145551025\n",
      "Iteration  147000  weights  [[0.34891128 0.47830507 0.28390749 0.48169535 0.2085624 ]\n",
      " [0.3560399  0.3881075  0.55636982 0.28114231 0.59061689]]  bias  [ 0.10809243 -0.07324618 -0.03847524  0.00570115 -0.00207216] loss 1.6127421431501832\n",
      "Iteration  148000  weights  [[0.34866094 0.47801751 0.28475525 0.48069902 0.20924886]\n",
      " [0.35638059 0.38836856 0.55560144 0.28217761 0.58974822]]  bias  [ 0.10794456 -0.07325886 -0.03856413  0.00571985 -0.00184142] loss 1.6126920171719035\n",
      "Iteration  149000  weights  [[0.34841265 0.477732   0.28559761 0.4797089  0.20993042]\n",
      " [0.35671932 0.38862803 0.55483824 0.28320612 0.5888847 ]]  bias  [ 0.10779706 -0.07327141 -0.03865287  0.00573838 -0.00161115] loss 1.6126425284889288\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAT0pJREFUeJzt3XlYVOX7BvB7QBhABGRTQQRzwz2S3LdSMTIUzTR3odLUMjOtzJ/mkhu5l0v6TTHTXEpJzTQ0FTVNUSm30AR3wIgAEQNi3t8fxMTIIgMzc2bOuT/XxVVz5j1nnucwwO17llEJIQSIiIiIFMRK6gKIiIiITI0BiIiIiBSHAYiIiIgUhwGIiIiIFIcBiIiIiBSHAYiIiIgUhwGIiIiIFIcBiIiIiBSHAYiIiIgUhwGIyMzNmDEDKpVK6jLM0unTp9G+fXtUrVoVKpUKcXFxUpdUIj8/P4wcOVLqMsp09epVBAUFwdnZGSqVClFRUSWOu379OlQqFSIjI01aX2UcPnwYKpUKhw8flroUMiMMQGQxIiMjoVKpEBsbK3UpZAby8vLw0ksvIS0tDUuWLMHGjRvh6+srWT0//fQTZsyYgfT0dMlqqIwRI0bg/PnzmDNnDjZu3IjAwMByr7t3717MmDHDeMWV08qVKy0qmJG0qkhdABFRRVy7dg03btzA2rVr8eqrr0pdDn766SfMnDkTI0eOhIuLi85z8fHxsLIy339vPnz4ECdOnMDUqVPxxhtvlDnW19cXDx8+hI2NjXbZ3r17sWLFCslD0MqVK+Hu7l5stq1z5854+PAhbG1tpSmMzBIDEBFZpHv37gFAsbBhjtRqtdQllOmPP/4AUL59qVKpYGdnZ+SKACEE/v77b9jb21d6W1ZWViapmSyL+f6ThKiCzp07h+DgYDg5OcHR0RHdunXDyZMndcbk5eVh5syZaNCgAezs7ODm5oaOHTsiOjpaOyY5ORlhYWGoXbs21Go1atWqhT59+uD69eulvvbChQuhUqlw48aNYs9NmTIFtra2+OuvvwAAR48exUsvvYQ6depArVbDx8cHb7/9Nh4+fFhmf2Wdg6FSqYr9K/zOnTsIDw9HjRo1oFar0bRpU6xbt67M1yi0fv16PPvss/D09IRarUaTJk2watWqYuNiY2PRs2dPuLu7w97eHnXr1kV4ePhjt//tt9+iV69e8PLyglqtRr169TB79mzk5+eXud7IkSPRpUsXAMBLL70ElUqFrl27AgC6du2q/f9H1/Hz89M+LtyPCxcuxJo1a1CvXj2o1Wo8/fTTOH36dLH1f/vtNwwYMAAeHh6wt7dHo0aNMHXqVAAF52lNnjwZAFC3bl2oVCqoVCrte6Wkc4ASEhLw0ksvwdXVFQ4ODmjbti2+++47nTGF565s27YNc+bMQe3atWFnZ4du3brh999/L3MfFXrcz8OMGTO0hw4nT54MlUqls58e9ej7b+TIkVixYgUAaPsues6aRqPB0qVL0bRpU9jZ2aFGjRoYPXq09uegkJ+fH1544QXs378fgYGBsLe3x2effQagfO9DPz8/XLx4EUeOHNHWUPg+KO0coO3bt6NVq1awt7eHu7s7hg4dijt37uiMGTlyJBwdHXHnzh2EhobC0dERHh4emDRp0mPfp2TeOANEsnLx4kV06tQJTk5OePfdd2FjY4PPPvsMXbt2xZEjR9CmTRsABb/0582bh1dffRWtW7dGZmYmYmNjcfbsWfTo0QMA8OKLL+LixYt488034efnh3v37iE6Oho3b94s9Q/EgAED8O6772Lbtm3aP4iFtm3bhqCgIFSvXh1AwS/f7OxsjBkzBm5ubjh16hQ++eQT3L59G9u3bzfI/khJSUHbtm2hUqnwxhtvwMPDA99//z1eeeUVZGZmYsKECWWuv2rVKjRt2hS9e/dGlSpVsHv3bowdOxYajQbjxo0DUDATExQUBA8PD7z//vtwcXHB9evXsWPHjsfWFxkZCUdHR0ycOBGOjo748ccfMX36dGRmZuLjjz8udb3Ro0fD29sbc+fOxfjx4/H000+jRo0aeu2bQps3b8b9+/cxevRoqFQqREREoF+/fkhISNAe5vn111/RqVMn2NjYYNSoUfDz88O1a9ewe/duzJkzB/369cOVK1fw1VdfYcmSJXB3dwcAeHh4lPiaKSkpaN++PbKzszF+/Hi4ublhw4YN6N27N77++mv07dtXZ/z8+fNhZWWFSZMmISMjAxERERgyZAh+/vnnMnsrz89Dv3794OLigrfffhuDBg3C888/D0dHx3Lvv9GjR+Pu3buIjo7Gxo0bS3w+MjISYWFhGD9+PBITE/Hpp5/i3LlzOH78uM6htPj4eAwaNAijR4/Ga6+9hkaNGgEo3/tw6dKlePPNN+Ho6KgNpmW9JwprevrppzFv3jykpKRg2bJlOH78OM6dO6czG5afn4+ePXuiTZs2WLhwIQ4cOIBFixahXr16GDNmTLn3FZkZQWQh1q9fLwCI06dPlzomNDRU2NraimvXrmmX3b17V1SrVk107txZu6xly5aiV69epW7nr7/+EgDExx9/rHed7dq1E61atdJZdurUKQFAfPHFF9pl2dnZxdadN2+eUKlU4saNG9plH374oSj6o5qYmCgAiPXr1xdbH4D48MMPtY9feeUVUatWLZGamqoz7uWXXxbOzs4l1lBUSc/37NlTPPHEE9rHO3fufOz3RZ/tjx49Wjg4OIi///67zHUPHTokAIjt27frLO/SpYvo0qVLsfEjRowQvr6+2seF+9HNzU2kpaVpl3/77bcCgNi9e7d2WefOnUW1atV0vi9CCKHRaLT///HHHwsAIjExsdhr+/r6ihEjRmgfT5gwQQAQR48e1S67f/++qFu3rvDz8xP5+fk6PTZu3Fjk5ORoxy5btkwAEOfPny955/yrvD8PhfuiPO/3kt5/48aNEyX9OTl69KgAIDZt2qSzfN++fcWW+/r6CgBi3759xbZTnvehEEI0bdq0xO994X48dOiQEEKI3Nxc4enpKZo1ayYePnyoHbdnzx4BQEyfPl27bMSIEQKAmDVrls42AwICiv2ck2XhITCSjfz8fPzwww8IDQ3FE088oV1eq1YtDB48GMeOHUNmZiaAgnMdLl68iKtXr5a4LXt7e9ja2uLw4cPFpuofZ+DAgThz5gyuXbumXbZ161ao1Wr06dNH5zUKPXjwAKmpqWjfvj2EEDh37pxer1kSIQS++eYbhISEQAiB1NRU7VfPnj2RkZGBs2fPlrmNojVmZGQgNTUVXbp0QUJCAjIyMgD8d97Inj17kJeXp1eNRbd///59pKamolOnTsjOzsZvv/2m17YqauDAgdpZOQDo1KkTgIJDVEDB+TExMTEIDw9HnTp1dNat6O0J9u7di9atW6Njx47aZY6Ojhg1ahSuX7+OS5cu6YwPCwvTOYH30RpLos/Pg7Fs374dzs7O6NGjh877r1WrVnB0dMShQ4d0xtetWxc9e/Ystp3yvA/1ERsbi3v37mHs2LE65wb16tUL/v7+xQ5FAsDrr7+u87hTp05l7n8yfwxAJBt//PEHsrOztdPmRTVu3BgajQa3bt0CAMyaNQvp6elo2LAhmjdvjsmTJ+PXX3/Vjler1ViwYAG+//571KhRA507d0ZERASSk5MfW8dLL70EKysrbN26FUBBENm+fbv2PIxCN2/exMiRI+Hq6qo9r6DwvJaK/FJ/1B9//IH09HSsWbMGHh4eOl9hYWEA/juRuDTHjx9H9+7dUbVqVbi4uMDDwwMffPCBTo1dunTBiy++iJkzZ8Ld3R19+vTB+vXrkZOT89gaL168iL59+8LZ2RlOTk7w8PDA0KFDdbZvbI+GmsIwVBh8C//INWvWzGCveePGjVLfp4XP61NjSfT5eTCWq1evIiMjA56ensXeg1lZWcXef3Xr1i1xO+V5H+qjcP+WtG/8/f2L7X87O7tihzOrV6+u9z+OyLzwHCBSpM6dO+PatWv49ttv8cMPP+B///sflixZgtWrV2svqZ4wYQJCQkIQFRWF/fv3Y9q0aZg3bx5+/PFHBAQElLptLy8vdOrUCdu2bcMHH3yAkydP4ubNm1iwYIF2TH5+Pnr06IG0tDS899578Pf3R9WqVXHnzh2MHDkSGo2m1O2XNuvw6AmZhdsYOnQoRowYUeI6LVq0KPV1rl27hm7dusHf3x+LFy+Gj48PbG1tsXfvXixZskS7fZVKha+//honT57E7t27sX//foSHh2PRokU4efJkqeeTpKeno0uXLnBycsKsWbNQr1492NnZ4ezZs3jvvffK3AdlUalUEEIUW17aCavW1tYlLi9pG1KxhBpLotFo4OnpiU2bNpX4/KOhoqQrvsr7PjSm0vY/WTYGIJINDw8PODg4ID4+vthzv/32G6ysrODj46Nd5urqirCwMISFhSErKwudO3fGjBkzdO4pU69ePbzzzjt45513cPXqVTz55JNYtGgRvvzyyzJrGThwIMaOHYv4+Hhs3boVDg4OCAkJ0T5//vx5XLlyBRs2bMDw4cO1y4tehVaawn/9P3rDvUf/1erh4YFq1aohPz8f3bt3f+x2H7V7927k5ORg165dOjMQjx62KNS2bVu0bdsWc+bMwebNmzFkyBBs2bKl1Hv0HD58GH/++Sd27NiBzp07a5cnJibqXWtR1atXL/HQRElX5pVH4eGjCxculDlOn8Nhvr6+pb5PC5+vLH1/HiqjtN7r1auHAwcOoEOHDhW+nF2f92F5vweF+zc+Ph7PPvusznPx8fGS3lCTTIeHwEg2rK2tERQUhG+//VbnUvWUlBRs3rwZHTt21B6C+vPPP3XWdXR0RP369bWHbbKzs/H333/rjKlXrx6qVatWrkM7L774IqytrfHVV19h+/bteOGFF1C1alWdWgHdf8ELIbBs2bLHbtvJyQnu7u6IiYnRWb5y5Uqdx9bW1njxxRfxzTfflPjHu/DeL6UpqcaMjAysX79eZ9xff/1VbCbiySefBIAy91VJ28/NzS3Wh77q1auH3377Tae/X375BcePH6/Q9jw8PNC5c2esW7cON2/e1HmuaO2F39/y3An6+eefx6lTp3DixAntsgcPHmDNmjXw8/NDkyZNKlRrUfr8PFRWab0PGDAA+fn5mD17drF1/vnnn3Ltq/K+DwvrKM82AwMD4enpidWrV+u8R7///ntcvnwZvXr1euw2yPJxBogszrp167Bv375iy9966y189NFHiI6ORseOHTF27FhUqVIFn332GXJychAREaEd26RJE3Tt2hWtWrWCq6srYmNj8fXXX2vvgnvlyhV069YNAwYMQJMmTVClShXs3LkTKSkpePnllx9bo6enJ5555hksXrwY9+/fx8CBA3We9/f3R7169TBp0iTcuXMHTk5O+Oabb8p9TsGrr76K+fPn49VXX0VgYCBiYmJw5cqVYuPmz5+PQ4cOoU2bNnjttdfQpEkTpKWl4ezZszhw4ADS0tJKfY2goCDY2toiJCQEo0ePRlZWFtauXQtPT08kJSVpx23YsAErV65E3759Ua9ePdy/fx9r166Fk5MTnn/++VK33759e1SvXh0jRozA+PHjoVKpsHHjxkof1gkPD8fixYvRs2dPvPLKK7h37x5Wr16Npk2bVvik3+XLl6Njx4546qmnMGrUKNStWxfXr1/Hd999p/38sVatWgEApk6dipdffhk2NjYICQnRCb6F3n//fXz11VcIDg7G+PHj4erqig0bNiAxMRHffPONwe4aXd6fh8oq7H38+PHo2bMnrK2t8fLLL6NLly4YPXo05s2bh7i4OAQFBcHGxgZXr17F9u3bsWzZMvTv37/MbZf3fVhYx6pVq/DRRx+hfv368PT0LDbDAwA2NjZYsGABwsLC0KVLFwwaNEh7Gbyfnx/efvttg+0bMmMSXHlGVCGFl8GX9nXr1i0hhBBnz54VPXv2FI6OjsLBwUE888wz4qefftLZ1kcffSRat24tXFxchL29vfD39xdz5swRubm5QgghUlNTxbhx44S/v7+oWrWqcHZ2Fm3atBHbtm0rd71r164VAES1atV0LrUtdOnSJdG9e3fh6Ogo3N3dxWuvvSZ++eWXYpcYP3oZvBAFlwW/8sorwtnZWVSrVk0MGDBA3Lt3r9hl8EIIkZKSIsaNGyd8fHyEjY2NqFmzpujWrZtYs2bNY3vYtWuXaNGihbCzsxN+fn5iwYIFYt26dTqXe589e1YMGjRI1KlTR6jVauHp6SleeOEFERsb+9jtHz9+XLRt21bY29sLLy8v8e6774r9+/frXLJcmtIugxdCiC+//FI88cQTwtbWVjz55JNi//79pV4GX9Kl3yXtxwsXLoi+ffsKFxcXYWdnJxo1aiSmTZumM2b27NnC29tbWFlZ6eyjRy+DF0KIa9euif79+2u317p1a7Fnz55y9VjWrRAeVZ6fh8peBv/PP/+IN998U3h4eAiVSlXs/bpmzRrRqlUrYW9vL6pVqyaaN28u3n33XXH37l3tGF9f31JvTVGe96EQQiQnJ4tevXqJatWqCQDaS+IfvQy+0NatW0VAQIBQq9XC1dVVDBkyRNy+fVtnzIgRI0TVqlWL1VTSzyVZFpUQZn4WHREREZGB8RwgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHN4IsQQajQZ3795FtWrVKvxpz0RERGRaQgjcv38fXl5ej72hKANQCe7evWuwz8ghIiIi07p16xZq165d5hgGoBJUq1YNQMEONNRn5RTKy8vDDz/8oL0lvNyxX3ljv/KmtH4B5fUst34zMzPh4+Oj/TteFgagEhQe9nJycjJKAHJwcICTk5Ms3myPw37ljf3Km9L6BZTXs1z7Lc/pKzwJmoiIiBSHAYiIiIgUhwGIiIiIFIcBiIiIiBSHAYiIiIgUhwGIiIiIFIcBiIiIiBRH0gAUExODkJAQeHl5QaVSISoq6rHr5OTkYOrUqfD19YVarYafnx/WrVunMyY9PR3jxo1DrVq1oFar0bBhQ+zdu9dIXRAREZGlkfRGiA8ePEDLli0RHh6Ofv36lWudAQMGICUlBZ9//jnq16+PpKQkaDQa7fO5ubno0aMHPD098fXXX8Pb2xs3btyAi4uLkbogIiIiSyNpAAoODkZwcHC5x+/btw9HjhxBQkICXF1dAQB+fn46Y9atW4e0tDT89NNP2rtaPjpGKjt3Av36WQEIASCwYwfQt6/UVRERESmPRZ0DtGvXLgQGBiIiIgLe3t5o2LAhJk2ahIcPH+qMadeuHcaNG4caNWqgWbNmmDt3LvLz8yWsHFCpgIJJLmsU7HZr9OtXsJyIiIhMy6I+CywhIQHHjh2DnZ0ddu7cidTUVIwdOxZ//vkn1q9frx3z448/YsiQIdi7dy9+//13jB07Fnl5efjwww9L3G5OTg5ycnK0jzMzMwEUfEZKXl5epeu2tbXCf1mzaOIRBUtUGuTmah5dTRYK958h9qMlYL/yxn7lT2k9y61fffpQCSGEEWspN5VKhZ07dyI0NLTUMUFBQTh69CiSk5Ph7OwMANixYwf69++PBw8ewN7eHg0bNsTff/+NxMREWFtbAwAWL16Mjz/+GElJSSVud8aMGZg5c2ax5Zs3b4aDg0Ol+jp5Epg/v3dhlyWMKNj977+/C23bVuqliIiIFC07OxuDBw9GRkbGYz/M3KJmgGrVqgVvb29t+AGAxo0bQwiB27dvo0GDBqhVqxZsbGy04adwTHJyMnJzc2Fra1tsu1OmTMHEiRO1jzMzM+Hj44OgoKBKfxp8aKgVSg4+hQqemz//BVnOAuXl5SE6Oho9evSQ1ScNl4b9yhv7lT+l9Sy3fguP4JSHRQWgDh06YPv27cjKyoKjoyMA4MqVK7CyskLt2rW1YzZv3gyNRgMrKyvtmFq1apUYfgBArVZDrVYXW25jY2PCN4Q1bGysHz/MQpl2X0qP/cob+5U/pfUsl3716UHSk6CzsrIQFxeHuLg4AEBiYiLi4uJw8+ZNAAUzM8OHD9eOHzx4MNzc3BAWFoZLly4hJiYGkydPRnh4OOzt7QEAY8aMQVpaGt566y1cuXIF3333HebOnYtx48aZvD8iIiIyT5IGoNjYWAQEBCAgIAAAMHHiRAQEBGD69OkAgKSkJG0YAgBHR0dER0cjPT0dgYGBGDJkCEJCQrB8+XLtGB8fH+zfvx+nT59GixYtMH78eLz11lt4//33TdscERERmS1JD4F17doVZZ2DHRkZWWyZv78/oqOjy9xuu3btcPLkycqWZ3Jr1wKvvSZ1FURERPJnUfcBkrtRo6SugIiISBkYgIiIiEhxGICMzFq+F3YRERFZLAYgI7t6VeoKiIiI6FEMQEZWt67UFRAREdGjGICIiIhIcRiAzMySJVJXQEREJH8MQGamyEeSERERkZEwABEREZHiMACZQJEPryciIiIzwABkArwUnoiIyLwwAJmAh4fUFRAREVFRDEBmKC1N6gqIiIjkjQHIDLm5SV0BERGRvDEAERERkeIwABEREZHiMACZSESE1BUQERFRIQYgE5k8WeoKiIiIqBADEBERESkOA5CZ2rRJ6gqIiIjkiwHITA0dKnUFRERE8sUARERERIrDAERERESKwwBkQocPA4CQuAoiIiJiADKhLl0ABiAiIiLpMQCZsfPnpa6AiIhInhiAzFiLFlJXQEREJE8MQERERKQ4DEAmx3OAiIiIpMYAZGLLlmnAEERERCQtBiATGzMGYAAiIiKSFgOQmUtLk7oCIiIi+WEAMnNublJXQEREJD8MQERERKQ4DECS4DlAREREUmIAkkDfvnvAEERERCQdBiAJjBgBMAARERFJhwHIAiQnS10BERGRvDAAWYBataSugIiISF4YgIiIiEhxGIAkw3OAiIiIpMIAJJFp0zRSl0BERKRYDEASmTZN6gqIiIiUiwHIQhw5InUFRERE8sEAZCG6dpW6AiIiIvlgACIiIiLFYQAiIiIixWEAktDKlVJXQEREpEwMQBIaM0bqCoiIiJSJAciCLFokdQVERETywABkQSZNkroCIiIieWAAIiIiIsVhACIiIiLFYQCSWGys1BUQEREpDwOQxFq10m98XJxRyiAiIlIUBiALExAgdQVERESWjwGIiIiIFIcBiIiIiBSHAcgMzJ0rdQVERETKwgBkBqZMkboCIiIiZWEAskD8DDEiIqLKkTQAxcTEICQkBF5eXlCpVIiKinrsOjk5OZg6dSp8fX2hVqvh5+eHdevWaZ+PjIyESqXS+bKzszNiF6a3erXUFRAREVm2KlK++IMHD9CyZUuEh4ejX79+5VpnwIABSElJweeff4769esjKSkJGo1GZ4yTkxPi4+O1j1UqlUHrJiIiIssmaQAKDg5GcHBwucfv27cPR44cQUJCAlxdXQEAfn5+xcapVCrUrFnTUGWaROPGwOXLUldBRESkDJIGIH3t2rULgYGBiIiIwMaNG1G1alX07t0bs2fPhr29vXZcVlYWfH19odFo8NRTT2Hu3Llo2rRpqdvNyclBTk6O9nFmZiYAIC8vD3l5eQbtoXB7j2735EnA2bkKgPLMVmmQl5dv0LqMpbR+5Yr9yhv7lT+l9Sy3fvXpQyWEEEaspdxUKhV27tyJ0NDQUsc899xzOHz4MLp3747p06cjNTUVY8eOxTPPPIP169cDAE6cOIGrV6+iRYsWyMjIwMKFCxETE4OLFy+idu3aJW53xowZmDlzZrHlmzdvhoODg0H6K4/Q0BCU77QsgRde2IVXXzV2RURERJYjOzsbgwcPRkZGBpycnMoca1EBKCgoCEePHkVycjKcnZ0BADt27ED//v3x4MEDnVmgQnl5eWjcuDEGDRqE2bNnl7jdkmaAfHx8kJqa+tgdqK+8vDxER0ejR48esLGx0XnO1tYa5T8vPR+5uZrHD5NYWf3KEfuVN/Yrf0rrWW79ZmZmwt3dvVwByKIOgdWqVQve3t7a8AMAjRs3hhACt2/fRoMGDYqtY2Njg4CAAPz++++lbletVkOtVpe4rrHeEJXftjVsbKwNVo+xGXNfmiP2K2/sV/6U1rNc+tWnB4u6D1CHDh1w9+5dZGVlaZdduXIFVlZWpR7eys/Px/nz51GrVi1TlUlERERmTtIAlJWVhbi4OMTFxQEAEhMTERcXh5s3bwIApkyZguHDh2vHDx48GG5ubggLC8OlS5cQExODyZMnIzw8XHv4a9asWfjhhx+QkJCAs2fPYujQobhx4wZetYATZn77TeoKiIiIlEHSABQbG4uAgAAEBAQAACZOnIiAgABMnz4dAJCUlKQNQwDg6OiI6OhopKenIzAwEEOGDEFISAiWL1+uHfPXX3/htddeQ+PGjfH8888jMzMTP/30E5o0aWLa5iqgUSP9xr/3nnHqICIikjtJzwHq2rUryjoHOzIystgyf39/REdHl7rOkiVLsGTJEkOUZ/YiIoAFC6SugoiIyPJY1DlARERERIbAAERERESKwwBkZi5elLoCIiIi+WMAMjP6nqs9bJhx6iAiIpIzBiAL9+WXUldARERkeRiAiIiISHEYgMxQvXpSV0BERCRvDEBm6Px5/cafOWOcOoiIiOSKAcgMlfCh9mUKDDROHURERHLFAERERESKwwBEREREisMAZKYmTZK6AiIiIvliADJTH3+s3/iZM41TBxERkRwxAMnEjBlSV0BERGQ5GICIiIhIcRiAiIiISHEYgMzYnj36jecNEYmIiMqHAciM9eql33jeEJGIiKh8GICIiIhIcRiAiIiISHEYgMzcG29IXQEREZH8MACZuU8+0W/8Sy8Zpw4iIiI5YQCSma+/lroCIiIi88cARERERIrDAGQB7OykroCIiEheGIAswM2b+o1/803j1EFERCQXDEAWwMNDv/GffmqcOoiIiOSCAYiIiIgUhwGIiIiIFIcByEIcPKjf+MmTjVMHERGRHDAAWYhnn9Vv/MKFxqmDiIhIDhiAiIiISHEYgIiIiEhxGIAsyPbt+o0fM8Y4dRAREVk6BiAL0r+/fuNXrzZOHURERJaOAYiIiIgUhwGIiIiIFIcByMLs2aPf+N69jVMHERGRJWMAsjC9euk3fvdu49RBRERkyRiAiIiISHEYgCyQSqXf+EuXjFMHERGRpWIAskDXruk3vmlT49RBRERkqRiALFDdulJXQEREZNkYgIiIiEhxGIAslL6Xt3/4oXHqICIiskQMQBbq22/1Gz9rlnHqICIiskQMQERERKQ4DEAK8vCh1BUQERGZBwYgC7ZwoX7jHRyMUwcREZGlYQCyYO+8I3UFRERElokBiIiIiBSHAcjCde6s3/gxY4xTBxERkSVhALJwR47oN371auPUQUREZEkYgIiIiEhxGIAUaNMmqSsgIiKSFgOQDKxcqd/4oUONUwcREZGlYACSAZ7YTEREpB8GIIW6c0fqCoiIiKTDACQT/fvrN752bePUQUREZAkYgGRi+3apKyAiIrIckgagmJgYhISEwMvLCyqVClFRUY9dJycnB1OnToWvry/UajX8/Pywbt26Esdu2bIFKpUKoaGhhi2ciIiILFoVKV/8wYMHaNmyJcLDw9GvX79yrTNgwACkpKTg888/R/369ZGUlASNRlNs3PXr1zFp0iR06tTJ0GWbrY4dgWPHyj8+MBCIjTVePUREROZK0gAUHByM4ODgco/ft28fjhw5goSEBLi6ugIA/Pz8io3Lz8/HkCFDMHPmTBw9ehTp6ekGqti8HT0KqFTlH3/mjPFqISIiMmeSBiB97dq1C4GBgYiIiMDGjRtRtWpV9O7dG7Nnz4a9vb123KxZs+Dp6YlXXnkFR48efex2c3JykJOTo32cmZkJAMjLy0NeXp5BeyjcnqG3+x9rlP/IpgZ5eflGqqOA8fs1L+xX3tiv/CmtZ7n1q08fFhWAEhIScOzYMdjZ2WHnzp1ITU3F2LFj8eeff2L9+vUAgGPHjuHzzz9HXFxcubc7b948zJw5s9jyH374AQ4ODoYqX0d0dLRRtmtj8zTy8rzKOVoFP7+9et9IsSKM1a+5Yr/yxn7lT2k9y6Xf7Ozsco9VCSGEEWspN5VKhZ07d5Z5wnJQUBCOHj2K5ORkODs7AwB27NiB/v3748GDB/jnn3/QokULrFy5UntobeTIkUhPTy/zBOuSZoB8fHyQmpoKJycng/RXKC8vD9HR0ejRowdsbGwMum0AyMoCXF2rACjvsbB85OYWP4fKUIzdr7lhv/LGfuVPaT3Lrd/MzEy4u7sjIyPjsX+/LWoGqFatWvD29taGHwBo3LgxhBC4ffs2Hjx4gOvXryMkJET7fOEJ0lWqVEF8fDzq1atXbLtqtRpqtbrYchsbG6O9IYy17erV9V3DGjY21gav41HG3JfmiP3KG/uVP6X1LJd+9enBogJQhw4dsH37dmRlZcHR0REAcOXKFVhZWaF27dpQqVQ4f/68zjr/93//h/v372PZsmXw8fGRomyTU6kAfeb1mjUDLlwwXj1ERETmRtIAlJWVhd9//137ODExEXFxcXB1dUWdOnUwZcoU3LlzB1988QUAYPDgwZg9ezbCwsIwc+ZMpKamYvLkyQgPD9eeBN2sWTOd13BxcSlxuZzduqXfnZ4vXjReLUREROZI0hshxsbGIiAgAAEBAQCAiRMnIiAgANOnTwcAJCUl4ebNm9rxjo6OiI6ORnp6OgIDAzFkyBCEhIRg+fLlktRvrry99V8nPt7wdRAREZkrSWeAunbtirLOwY6MjCy2zN/fX6+z1UvahhK4uAD63P7I31+/w2ZERESWjJ8FJlN370pdARERkfliAJKpIveFLLcVKwxfBxERkTliAJKxp57Sb/wbbxinDiIiInPDACRj/KwvIiKikjEAkY4+faSugIiIyPgYgGTu308EKbddu4xTBxERkTlhAJK5vXulroCIiMj8MABRMXXqSF0BERGRcTEAKcCsWfqNv3XLOHUQERGZiwoFoFu3buH27dvax6dOncKECROwZs0agxVGhjNtmv7rvPOO4esgIiIyFxUKQIMHD8ahQ4cAAMnJyejRowdOnTqFqVOnYpa+0w1kEk5O+o1fvNg4dRAREZmDCgWgCxcuoHXr1gCAbdu2oVmzZvjpp5+wadMmxX72lrn74w+pKyAiIjIfFQpAeXl5UKvVAIADBw6gd+/eAAo+qDQpKclw1ZHB2Nrqv07t2oavg4iIyBxUKAA1bdoUq1evxtGjRxEdHY3nnnsOAHD37l24ubkZtEAyHH1vcnjnjnHqICIiklqFAtCCBQvw2WefoWvXrhg0aBBatmwJANi1a5f20BiZn6go/dfp39/gZRAREUmuSkVW6tq1K1JTU5GZmYnq1atrl48aNQoODg4GK46k9803UldARERkeBWaAXr48CFycnK04efGjRtYunQp4uPj4enpadACybAqcopWdLTh6yAiIpJShQJQnz598MUXXwAA0tPT0aZNGyxatAihoaFYtWqVQQskw6pZU/91goIMXwcREZGUKhSAzp49i06dOgEAvv76a9SoUQM3btzAF198geXLlxu0QDK8oUOlroCIiEhaFQpA2dnZqFatGgDghx9+QL9+/WBlZYW2bdvixo0bBi2QDG/jRv3X8fIyfB1ERERSqVAAql+/PqKionDr1i3s378fQf8eI7l37x6c9L3lMEmiXj39xvP2TkREJCcVCkDTp0/HpEmT4Ofnh9atW6Ndu3YACmaDAgICDFogGcfvv+u/zr/fZiIiIotXocvg+/fvj44dOyIpKUl7DyAA6NatG/r27Wuw4si8nDwpdQVERESGUaEZIACoWbMmAgICcPfuXe0nw7du3Rr+/v4GK46M6+ef9V8nNNTgZRAREZlchQKQRqPBrFmz4OzsDF9fX/j6+sLFxQWzZ8+GRqMxdI1kJBW5afe33xq+DiIiIlOrUACaOnUqPv30U8yfPx/nzp3DuXPnMHfuXHzyySeYNm2aoWskI3r/ff3XGT3a8HUQERGZUoUC0IYNG/C///0PY8aMQYsWLdCiRQuMHTsWa9euRWRkpIFLJGOaN0//ddasMXwdREREplShAJSWllbiuT7+/v5IS0urdFFkWr166b/O5MmGr4OIiMhUKhSAWrZsiU8//bTY8k8//RQtWrSodFFkWnv26L/OwoWGr4OIiMhUKnQZfEREBHr16oUDBw5o7wF04sQJ3Lp1C3v37jVogWQa7doBJ07ot87bbwNLlhinHiIiImOq0AxQly5dcOXKFfTt2xfp6elIT09Hv379cPHiRWysyOcskOR++kn/dZYuNXgZREREJlGhGSAA8PLywpw5c3SW/fLLL/j888+xhmfJWqQWLYBff9VvnUGDgK++Mk49RERExlLhGyGS/Pzyi/7rbNli+DqIiIiMjQGIdDRtqv86zz5r+DqIiIiMiQGIdFy4oP86hw4Zvg4iIiJj0uscoH79+pX5fHp6emVqITPRqhVw5ox+69SrB1y7Zpx6iIiIDE2vAOTs7PzY54cPH16pgkh6sbGASqXfOgkJxqmFiIjIGPQKQOvXrzdWHWRmunUDDh7Ubx2VChDCOPUQEREZEs8BohIdOFCx9aKjDVsHERGRMTAAUalGjdJ/naAgw9dBRERkaAxAVKrPPqvYesHBhq2DiIjI0BiAqEzff6//Ovv2Gb4OIiIiQ2IAojI991zF1tP3KjIiIiJTYgCix6rolV1btxq2DiIiIkNhAKJy8ffXf52XXzZ8HURERIbAAETlcvlyxdazs+NbjIiIzA//OlG5zZih/zoajRUyMw1eChERUaUwAFG5ffhhRdZSYfjwFwxdChERUaUwAJFeKnZCtBVsbflWIyIi88G/SqS3zp31XUMFwArx8UYohoiIqAIYgEhvR45UZC1Vha4kIyIiMgYGIKqQit4biDdIJCIic8AARBX25JMVW+/ttw1aBhERkd4YgKjCzp2r2HpLlxq0DCIiIr0xAFGl8FAYERFZIgYgqrTu3Su2HkMQERFJhQGIKi06uuLrvvaa4eogIiIqLwYgMoiKHgr73/8MWwcREVF5MACRwXzxRcXW46EwIiIyNQYgMphhwyq+LkMQERGZkqQBKCYmBiEhIfDy8oJKpUJUVNRj18nJycHUqVPh6+sLtVoNPz8/rFu3Tvv8jh07EBgYCBcXF1StWhVPPvkkNm7caMQuqKiKHgoDGIKIiMh0qkj54g8ePEDLli0RHh6Ofv36lWudAQMGICUlBZ9//jnq16+PpKQkaDQa7fOurq6YOnUq/P39YWtriz179iAsLAyenp7o2bOnsVqhIoSoeJhRqSoXooiIiMpD0gAUHByM4ODgco/ft28fjhw5goSEBLi6ugIA/Pz8dMZ07dpV5/Fbb72FDRs24NixYwxAJnTvHuDpWbF1rayAIpmWiIjI4CzqHKBdu3YhMDAQERER8Pb2RsOGDTFp0iQ8fPiwxPFCCBw8eBDx8fHorP9HmFMleHgU/p/+0zlCADdvGrQcIiIiHZLOAOkrISEBx44dg52dHXbu3InU1FSMHTsWf/75J9avX68dl5GRAW9vb+Tk5MDa2horV65Ejx49St1uTk4OcnJytI8zMzMBAHl5ecjLyzNoD4XbM/R2zVFuLmBra4WCnK3fMTFf33zk5lreNJCSvr8A+5U7pfULKK9nufWrTx8qIczjjAuVSoWdO3ciNDS01DFBQUE4evQokpOT4ezsDKDgpOf+/fvjwYMHsLe3BwBoNBokJCQgKysLBw8exOzZsxEVFVXs8FihGTNmYObMmcWWb968GQ4ODpXuTelCQ1+A/iFIANAgKmqPcYoiIiLZyc7OxuDBg5GRkQEnJ6cyx1pUABoxYgSOHz+O33//Xbvs8uXLaNKkCa5cuYIGDRqUuN6rr76KW7duYf/+/SU+X9IMkI+PD1JTUx+7A/WVl5eH6Oho9OjRAzY2Ngbdtjkq7LcyIciSZoKU+v1lv/KktH4B5fUst34zMzPh7u5ergBkUYfAOnTogO3btyMrKwuOjo4AgCtXrsDKygq1a9cudT2NRqMTcB6lVquhVquLLbexsTHaG8KY2zZHubka2Npa67mWCoA1bG2tLe7KMKV9f9mvvCmtX0B5PculX316kPQk6KysLMTFxSEuLg4AkJiYiLi4ONz89wzYKVOmYPjw4drxgwcPhpubG8LCwnDp0iXExMRg8uTJCA8P1x7+mjdvHqKjo5GQkIDLly9j0aJF2LhxI4YOHWry/kgX7xFERETmQtIZoNjYWDzzzDPaxxMnTgRQcKgrMjISSUlJ2jAEAI6OjoiOjsabb76JwMBAuLm5YcCAAfjoo4+0Yx48eICxY8fi9u3bsLe3h7+/P7788ksMHDjQdI1RqX74AQgKqti6vEcQEREZiqQBqGvXrijrFKTIyMhiy/z9/RFdxsePf/TRRzqBiMxLGRfjlQtDEBERGYJF3QeI5KGyAYaHw4iIqLIYgEgShghBu3YZphYiIlIeBiCSTGVDUJ8+nA0iIqKKYQAiSRnifB6GICIi0hcDEEmOIYiIiEyNAYjMAkMQERGZEgMQmQ1DhaA7dyq/HSIikjcGIDIrhghBtWtzNoiIiMrGAERmx1A3OmQIIiKi0jAAkVliCCIiImNiACKzZcgQ1KuXYbZFRETywABEZk0I4LffKr+dvXs5G0RERP9hACKz16iRYWeDMjIMsy0iIrJcDEBkMQwVglxcOBtERKR0DEBkUQwVggCGICIiJWMAIotj6BDk42O47RERkWVgACKLJAQwYIBhtnX7NmeDiIiUhgGILNbWrYafDbK3N9z2iIjIfDEAkcUzZAj6+++CIJSWZrhtEhGR+WEAIlkwZAgCADc3HhYjIpIzBiCSDSGAyEjDblOlAnr0MOw2iYhIegxAJCsjRhh+NujAAc4GERHJDQMQyZIQgLe3YbepUjEIERHJBQMQydbt24afDQIKQlDDhobfLhERmQ4DEMmeMULQ1asFQWjFCsNvm4iIjI8BiBRBCGDdOsNv9403eFiMiMgSMQCRYoSFGWc2COD5QUREloYBiBTHWLNBAGBra4XQUONsm4iIDIcBiBTJeLNB1gB6w9bWijNCRERmjAGIFE0IYwQhFQqCUMFhMS8vQ2+fiIgqiwGICAUhqE4d42w7KakgCDVtapztExGR/hiAiP5144bxTpIGgEuXCoJQQIDxXoOIiMqHAYjoEUIA339vvO3HxRUEoWbNjPcaRERUNgYgohI891xBEPL1Nd5rXLxYEISMdeiNiIhKxwBEVIbr1417WAwAbt0qCEJVqhj3dYiI6D8MQETlYJyrxXTl5/OGikREpsIARKQHIYAvvzT+6zAIEREZFwMQkZ6GDCkIQs8+a/zXYhAiIjIOBiCiCjp4sCAIuboa/7UKg9CuXcZ/LSIiJWAAIqqkP/8sen5QPgDjnSzUp09BEGrVymgvQUSkCAxARAYiBJCbqwGwCwVByHjOnuXhMSKiymAAIjKwqKjCIGQaDEJERPpjACIyElNcOl9UYRAaNMh0r0lEZKkYgIiMzNRBaMsWzgoRET0OAxCRiZg6CAEMQkREpWEAIjIxKYOQt7dpX5eIyFwxABFJpDAIVa1qute8e5ezQkREAAMQkeSysgqCULt2pn3dwiDUooVpX5eIyBwwABGZiZ9+KghCS5ea9nXPn+esEBEpDwMQkZl56y1pzhMC/gtCHh6mf20iIlNiACIyY1IFodRUzgoRkbwxABFZgMIg5OJi+tcuDEIMQ0QkJwxARBbkr7+kOU+oUGEQevZZaV6fiMhQGICILJCU5wkBwKFDBUHI1laa1yciqiwGICILJ8X9hP5TBaGhIbC1teIhMiKyKAxARDJReD+h334z5auqUPBrxLrg0b+HyKQJY0RE5ccARCQzjRpJe3gMALKz/wtDffpIVwcRUWkYgIhkrDAIffCBdDXs2vVfGDp2TLo6iIiKYgAiUoA5c6SfFQKATp14ST0RmQcGICKFKQxCM2ZIWwfvL0REUmIAIlKoDz80j1khgGGIiExP0gAUExODkJAQeHl5QaVSISoq6rHr5OTkYOrUqfD19YVarYafnx/WrVunfX7t2rXo1KkTqlevjurVq6N79+44deqUEbsgsnyFQSgnR+pKGIaIyDQkDUAPHjxAy5YtsWLFinKvM2DAABw8eBCff/454uPj8dVXX6FRo0ba5w8fPoxBgwbh0KFDOHHiBHx8fBAUFIQ7d+4YowUiWbG1/S8MeXtLXQ3DEBEZTxUpXzw4OBjBwcHlHr9v3z4cOXIECQkJcHV1BQD4+fnpjNm0aZPO4//973/45ptvcPDgQQwfPrzSNRMpxe3b//2/OQSQojWYw2E7IrJskgYgfe3atQuBgYGIiIjAxo0bUbVqVfTu3RuzZ8+Gvb19ietkZ2cjLy9PG5hKkpOTg5wic/+ZmZkAgLy8POTl5Rm0h8LtGXq75or9ykNubsF/L10CnnzSCgU3QCz8MjUBlUoAEAA02tpMQa7f39IorV9AeT3LrV99+lAJYR7/llKpVNi5cydCQ0NLHfPcc8/h8OHD6N69O6ZPn47U1FSMHTsWzzzzDNavX1/iOmPHjsX+/ftx8eJF2NnZlThmxowZmDlzZrHlmzdvhoODQ4X6IZK70aOBlJQXIHUYKvgq+P8PPtiD1q0lKIOIzEJ2djYGDx6MjIwMODk5lTnWogJQUFAQjh49iuTkZDg7OwMAduzYgf79++PBgwfFZoHmz5+PiIgIHD58GC1atCh1uyXNAPn4+CA1NfWxO1BfeXl5iI6ORo8ePWBjY2PQbZsj9itvhf2GhvYAoIa0YQj4LxAJPPusBvv2GXbrSv3+KqVfQHk9y63fzMxMuLu7lysAWdQhsFq1asHb21sbfgCgcePGEELg9u3baNCggXb5woULMX/+fBw4cKDM8AMAarUaarW62HIbGxujvSGMuW1zxH7lLTfXBjY2/30emHT+C18//mit82n1hvynntK+v0rrF1Bez3LpV58eLOo+QB06dMDdu3eRlZWlXXblyhVYWVmhdu3a2mURERGYPXs29u3bh8DAQClKJVKswqvIzGNu+T+8ooyIipI0AGVlZSEuLg5xcXEAgMTERMTFxeHmzZsAgClTpuhcuTV48GC4ubkhLCwMly5dQkxMDCZPnozw8HDt4a8FCxZg2rRpWLduHfz8/JCcnIzk5GSd0EREplEYhPbskboSXUXD0CMXjhKRQkgagGJjYxEQEICAgAAAwMSJExEQEIDp06cDAJKSkrRhCAAcHR0RHR2N9PR0BAYGYsiQIQgJCcHy5cu1Y1atWoXc3Fz0798ftWrV0n4tXLjQtM0RkVavXv+FoYkTpa5G19Ch/4UhK4uaEyeiypD0HKCuXbuirHOwIyMjiy3z9/dHdHR0qetcv37dAJURkbEsWlTwBQB9+wLluAG8yQjB+w0RKQX/vUNEktm587+ZIT3uiWoyRQ+V+ftLXQ0RGZJFXQVGRPK1d+9//29uM0MAEB8P2NpaA3gBgIazQ0QWjjNARGR2is4MTZokdTVFWQGwBlBFZ3aohLtoEJGZYwAiIrP28cf/haGDB6WuppDutfS5ubqHy777TqKyiKjceAiMiCzGs8/qnphsrvf0eeEF3cc8XEZkfhiAiMhiWUIYAorXxkBEJD0eAiMiWTDXO1CXpOjhMn54K5E0GICISHaKhqE+faSupmynT+sGorfflroiImVgACIiWYuKsqzZoaVLdQPR+vVSV0QkTwxARKQolhSGACA8XDcQbdwodUVE8sAARESKVTQMFflIQbM2fLhuIJoyReqKiCwTAxAREYA337S82SEAmD9fNxC1by91RUSWgQGIiKgERcPQ4sWFSzX/fplvQjpxQjcQmfPtAYikxABERPQYb79dEIRyc/MRFbUbwD9Sl6QXBiKi4hiAiIj0lJtrmYfLCj0aiGbOlLoiItNjACIiqqSiYejRj8GwBDNmcJaIlIcBiIjIgHbvtuzZoUIqFWBra43Q0BDY2lphxQqpKyIyLAYgIiIjKhqGLC8QWf37ZY033uAsEckLAxARkQlZdiDS9WggsreXuiKi8mMAIiKSkJwC0d9/Fw9Fzz8vdVVEJWMAIiIyI3IKRADw/ffFQ9GHH0pdFREDEBGRWZNbIAKAWbOKh6Jp06SuipSGAYiIyILIMRABwEcfFQ9Fzz4rdVUkZ1WkLoCIiCru0RAkp6uzDh0quR85BT+SDmeAiIhk5NEZov79pa7I8B6dKZJT6CPTYQAiIpKx7dvle9isqJJCUWCg1FWROWMAIiJSmEcDkZub1BUZx5kznC2i0vEcICIihUtNLb6sICjkAyhMDPL593JpIUius2NUMvm8o4mIyGCEAHJzNYiK2o3c3Hx06yZ1Rcan+/ln1pwxkjnOABER0WMdOFB8mTzDQfF5Ac4YyRMDEBERVUhJAUCeoahkDEaWjYfAiIjIYB49wVqJYaCkE69VKmDrVqkro6IYgIiIyKgYigq8/HLp4YhMjwGIiIhMjqFIV2nBqGVLqSuTLwYgIiIyCwxFxf36a+nhKCRE6uosGwMQERGZrZJC0QcfSF2Vedizp/Rw1KCB1NWZPwYgIiKyKHPmcLbocX7/vfRwxHOOCjAAERGRLJQUirZskboq81QYhEq68WPXrlJXZxoMQEREJFsDB5YcjEo/RKS0qSSrIl8Fjhwpe/Zo7VrJijUoBiAiIlKcK1dKO4z2Dwo+A00jbYFmbNSosgPSwIFSV1g+DEBERET/ys0FoqL2IDc3XycY2dhIXZnl2Lat7IBkLucgMQARERE9Rm5uyTNGSviQWGMoGoZ695amBn4WGBERUQWV9CGxhcxlpsPc7d5dsK9MfSUfZ4CIiIiMoKQZIyEALy+pKzNPpg6MDEBEREQmdOdO6eFI6Ux5OIyHwIiIiMxEWSFICYfUdu823WsxABEREVkApYcjQ+MhMCIiIgtX2iE1IYA2bcpaUwPl3fyxAAMQERGRjJ08WXo4ys3NB7ALBTd/lJ4pP+GeAYiIiEjBoqKA3FyNWZyYvWuX6V6L5wARERFRqR4Xggx1/hHvA0REREQWo6zzj4QAPD3LXj8kRJpbAHAGiIiIiIwmJUXqCkrGGSAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgIiIiUhwGICIiIlIc3gm6BOLfe3JnZmYafNt5eXnIzs5GZmYmbGxsDL59c8N+5Y39ypvS+gWU17Pc+i38uy3K8dkaDEAluH//PgDAx8dH4kqIiIhIX/fv34ezs3OZY1SiPDFJYTQaDe7evYtq1apBZaiPuf1XZmYmfHx8cOvWLTg5ORl02+aI/cob+5U3pfULKK9nufUrhMD9+/fh5eUFK6uyz/LhDFAJrKysULt2baO+hpOTkyzebOXFfuWN/cqb0voFlNeznPp93MxPIZ4ETURERIrDAERERESKwwBkYmq1Gh9++CHUarXUpZgE+5U39itvSusXUF7PSuu3KJ4ETURERIrDGSAiIiJSHAYgIiIiUhwGICIiIlIcBiAiIiJSHAYgE1qxYgX8/PxgZ2eHNm3a4NSpU1KXVMy8efPw9NNPo1q1avD09ERoaCji4+N1xvz9998YN24c3Nzc4OjoiBdffBEpKSk6Y27evIlevXrBwcEBnp6emDx5Mv755x+dMYcPH8ZTTz0FtVqN+vXrIzIyslg9pt5n8+fPh0qlwoQJE7TL5NbvnTt3MHToULi5ucHe3h7NmzdHbGys9nkhBKZPn45atWrB3t4e3bt3x9WrV3W2kZaWhiFDhsDJyQkuLi545ZVXkJWVpTPm119/RadOnWBnZwcfHx9EREQUq2X79u3w9/eHnZ0dmjdvjr179xq01/z8fEybNg1169aFvb096tWrh9mzZ+t8TpCl9xsTE4OQkBB4eXlBpVIhKipK53lz6q88tVSm37y8PLz33nto3rw5qlatCi8vLwwfPhx3796VZb+Pev3116FSqbB06VKL7dekBJnEli1bhK2trVi3bp24ePGieO2114SLi4tISUmRujQdPXv2FOvXrxcXLlwQcXFx4vnnnxd16tQRWVlZ2jGvv/668PHxEQcPHhSxsbGibdu2on379trn//nnH9GsWTPRvXt3ce7cObF3717h7u4upkyZoh2TkJAgHBwcxMSJE8WlS5fEJ598IqytrcW+ffu0Y0y9z06dOiX8/PxEixYtxFtvvSXLftPS0oSvr68YOXKk+Pnnn0VCQoLYv3+/+P3337Vj5s+fL5ydnUVUVJT45ZdfRO/evUXdunXFw4cPtWOee+450bJlS3Hy5Elx9OhRUb9+fTFo0CDt8xkZGaJGjRpiyJAh4sKFC+Krr74S9vb24rPPPtOOOX78uLC2thYRERHi0qVL4v/+7/+EjY2NOH/+vMH6nTNnjnBzcxN79uwRiYmJYvv27cLR0VEsW7ZMNv3u3btXTJ06VezYsUMAEDt37tR53pz6K08tlek3PT1ddO/eXWzdulX89ttv4sSJE6J169aiVatWOtuQS79F7dixQ7Rs2VJ4eXmJJUuWWGy/psQAZCKtW7cW48aN0z7Oz88XXl5eYt68eRJW9Xj37t0TAMSRI0eEEAW/YGxsbMT27du1Yy5fviwAiBMnTgghCn5graysRHJysnbMqlWrhJOTk8jJyRFCCPHuu++Kpk2b6rzWwIEDRc+ePbWPTbnP7t+/Lxo0aCCio6NFly5dtAFIbv2+9957omPHjqU+r9FoRM2aNcXHH3+sXZaeni7UarX46quvhBBCXLp0SQAQp0+f1o75/vvvhUqlEnfu3BFCCLFy5UpRvXp1bf+Fr92oUSPt4wEDBohevXrpvH6bNm3E6NGjK9dkEb169RLh4eE6y/r16yeGDBkihJBfv4/+gTSn/spTS2X7LcmpU6cEAHHjxg3Z9nv79m3h7e0tLly4IHx9fXUCkCX3a2w8BGYCubm5OHPmDLp3765dZmVlhe7du+PEiRMSVvZ4GRkZAABXV1cAwJkzZ5CXl6fTi7+/P+rUqaPt5cSJE2jevDlq1KihHdOzZ09kZmbi4sWL2jFFt1E4pnAbpt5n48aNQ69evYrVJLd+d+3ahcDAQLz00kvw9PREQEAA1q5dq30+MTERycnJOnU4OzujTZs2Ov26uLggMDBQO6Z79+6wsrLCzz//rB3TuXNn2Nra6vQbHx+Pv/76SzumrH1iCO3bt8fBgwdx5coVAMAvv/yCY8eOITg4WJb9Psqc+itPLcaQkZEBlUoFFxcXbZ1y6lej0WDYsGGYPHkymjZtWux5ufVrSAxAJpCamor8/HydP5AAUKNGDSQnJ0tU1eNpNBpMmDABHTp0QLNmzQAAycnJsLW11f4yKVS0l+Tk5BJ7LXyurDGZmZl4+PChSffZli1bcPbsWcybN6/Yc3LrNyEhAatWrUKDBg2wf/9+jBkzBuPHj8eGDRt06i2rjuTkZHh6euo8X6VKFbi6uhpknxiy3/fffx8vv/wy/P39YWNjg4CAAEyYMAFDhgzRqUUu/T7KnPorTy2G9vfff+O9997DoEGDtB/0Kbd+FyxYgCpVqmD8+PElPi+3fg2JnwZPpRo3bhwuXLiAY8eOSV2K0dy6dQtvvfUWoqOjYWdnJ3U5RqfRaBAYGIi5c+cCAAICAnDhwgWsXr0aI0aMkLg6w9u2bRs2bdqEzZs3o2nTpoiLi8OECRPg5eUly37pP3l5eRgwYACEEFi1apXU5RjFmTNnsGzZMpw9exYqlUrqciwOZ4BMwN3dHdbW1sWuHEpJSUHNmjUlqqpsb7zxBvbs2YNDhw6hdu3a2uU1a9ZEbm4u0tPTdcYX7aVmzZol9lr4XFljnJycYG9vb7J9dubMGdy7dw9PPfUUqlSpgipVquDIkSNYvnw5qlSpgho1asiq31q1aqFJkyY6yxo3boybN2/q1FtWHTVr1sS9e/d0nv/nn3+QlpZmkH1iyH4nT56snQVq3rw5hg0bhrfffls72ye3fh9lTv2VpxZDKQw/N27cQHR0tHb2p7AOufR79OhR3Lt3D3Xq1NH+/rpx4wbeeecd+Pn5aeuQS7+GxgBkAra2tmjVqhUOHjyoXabRaHDw4EG0a9dOwsqKE0LgjTfewM6dO/Hjjz+ibt26Os+3atUKNjY2Or3Ex8fj5s2b2l7atWuH8+fP6/zQFf4SKvzj265dO51tFI4p3Iap9lm3bt1w/vx5xMXFab8CAwMxZMgQ7f/Lqd8OHToUu63BlStX4OvrCwCoW7cuatasqVNHZmYmfv75Z51+09PTcebMGe2YH3/8ERqNBm3atNGOiYmJQV5enk6/jRo1QvXq1bVjytonhpCdnQ0rK91fc9bW1tBoNLLs91Hm1F95ajGEwvBz9epVHDhwAG5ubjrPy6nfYcOG4ddff9X5/eXl5YXJkydj//79suvX4KQ+C1sptmzZItRqtYiMjBSXLl0So0aNEi4uLjpXDpmDMWPGCGdnZ3H48GGRlJSk/crOztaOef3110WdOnXEjz/+KGJjY0W7du1Eu3bttM8XXhYeFBQk4uLixL59+4SHh0eJl4VPnjxZXL58WaxYsaLEy8Kl2GdFrwKTW7+nTp0SVapUEXPmzBFXr14VmzZtEg4ODuLLL7/Ujpk/f75wcXER3377rfj1119Fnz59SrxsOiAgQPz888/i2LFjokGDBjqX1aanp4saNWqIYcOGiQsXLogtW7YIBweHYpfVVqlSRSxcuFBcvnxZfPjhhwa/DH7EiBHC29tbexn8jh07hLu7u3j33Xdl0+/9+/fFuXPnxLlz5wQAsXjxYnHu3DntVU/m1F95aqlMv7m5uaJ3796idu3aIi4uTud3WNErnOTSb0kevQrM0vo1JQYgE/rkk09EnTp1hK2trWjdurU4efKk1CUVA6DEr/Xr12vHPHz4UIwdO1ZUr15dODg4iL59+4qkpCSd7Vy/fl0EBwcLe3t74e7uLt555x2Rl5enM+bQoUPiySefFLa2tuKJJ57QeY1CUuyzRwOQ3PrdvXu3aNasmVCr1cLf31+sWbNG53mNRiOmTZsmatSoIdRqtejWrZuIj4/XGfPnn3+KQYMGCUdHR+Hk5CTCwsLE/fv3dcb88ssvomPHjkKtVgtvb28xf/78YrVs27ZNNGzYUNja2oqmTZuK7777zqC9ZmZmirfeekvUqVNH2NnZiSeeeEJMnTpV54+hpfd76NChEn9mR4wYYXb9laeWyvSbmJhY6u+wQ4cOya7fkpQUgCypX1NSCVHklqhERERECsBzgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiIhIcRiAiIiISHEYgIiIiEhxGICIiP7l5+eHpUuXSl0GEZkAAxARSWLkyJEIDQ0FAHTt2hUTJkww2WtHRkbCxcWl2PLTp09j1KhRJquDiKRTReoCiIgMJTc3F7a2thVe38PDw4DVEJE54wwQEUlq5MiROHLkCJYtWwaVSgWVSoXr168DAC5cuIDg4GA4OjqiRo0aGDZsGFJTU7Xrdu3aFW+88QYmTJgAd3d39OzZEwCwePFiNG/eHFWrVoWPjw/Gjh2LrKwsAMDhw4cRFhaGjIwM7evNmDEDQPFDYDdv3kSfPn3g6OgIJycnDBgwACkpKdrnZ8yYgSeffBIbN26En58fnJ2d8fLLL+P+/fvaMV9//TWaN28Oe3t7uLm5oXv37njw4IGR9iYRlRcDEBFJatmyZWjXrh1ee+01JCUlISkpCT4+PkhPT8ezzz6LgIAAxMbGYt++fUhJScGAAQN01t+wYQNsbW1x/PhxrF69GgBgZWWF5cuX4+LFi9iwYQN+/PFHvPvuuwCA9u3bY+nSpXByctK+3qRJk4rVpdFo0KdPH6SlpeHIkSOIjo5GQkICBg4cqDPu2rVriIqKwp49e7Bnzx4cOXIE8+fPBwAkJSVh0KBBCA8Px+XLl3H48GH069cP/AhGIunxEBgRScrZ2Rm2trZwcHBAzZo1tcs//fRTBAQEYO7cudpl69atg4+PD65cuYKGDRsCABo0aICIiAidbRY9n8jPzw8fffQRXn/9daxcuRK2trZwdnaGSqXSeb1HHTx4EOfPn0diYiJ8fHwAAF988QWaNm2K06dP4+mnnwZQEJQiIyNRrVo1AMCwYcNw8OBBzJkzB0lJSfjnn3/Qr18/+Pr6AgCaN29eib1FRIbCGSAiMku//PILDh06BEdHR+2Xv78/gIJZl0KtWrUqtu6BAwfQrVs3eHt7o1q1ahg2bBj+/PNPZGdnl/v1L1++DB8fH234AYAmTZrAxcUFly9f1i7z8/PThh8AqFWrFu7duwcAaNmyJbp164bmzZvjpZdewtq1a/HXX3+VfycQkdEwABGRWcrKykJISAji4uJ0vq5evYrOnTtrx1WtWlVnvevXr+OFF15AixYt8M033+DMmTNYsWIFgIKTpA3NxsZG57FKpYJGowEAWFtbIzo6Gt9//z2aNGmCTz75BI0aNUJiYqLB6yAi/TAAEZHkbG1tkZ+fr7PsqaeewsWLF+Hn54f69evrfD0aeoo6c+YMNBoNFi1ahLZt26Jhw4a4e/fuY1/vUY0bN8atW7dw69Yt7bJLly4hPT0dTZo0KXdvKpUKHTp0wMyZM3Hu3DnY2tpi586d5V6fiIyDAYiIJOfn54eff/4Z169fR2pqKjQaDcaNG4e0tDQMGjQIp0+fxrVr17B//36EhYWVGV7q16+PvLw8fPLJJ0hISMDGjRu1J0cXfb2srCwcPHgQqampJR4a6969O5o3b44hQ4bg7NmzOHXqFIYPH44uXbogMDCwXH39/PPPmDt3LmJjY3Hz5k3s2LEDf/zxBxo3bqzfDiIig2MAIiLJTZo0CdbW1mjSpAk8PDxw8+ZNeHl54fjx48jPz0dQUBCaN2+OCRMmwMXFBVZWpf/qatmyJRYvXowFCxagWbNm2LRpE+bNm6czpn379nj99dcxcOBAeHh4FDuJGiiYufn2229RvXp1dO7cGd27d8cTTzyBrVu3lrsvJycnxMTE4Pnnn0fDhg3xf//3f1i0aBGCg4PLv3OIyChUgtdjEhERkcJwBoiIiIgUhwGIiIiIFIcBiIiIiBSHAYiIiIgUhwGIiIiIFIcBiIiIiBSHAYiIiIgUhwGIiIiIFIcBiIiIiBSHAYiIiIgUhwGIiIiIFIcBiIiIiBTn/wFXaQgPtIjXTwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SoftMax(training_features, training_labels , learning_rate=0.0001)\n",
    "model.train(150000,True)\n",
    "\n",
    "values = model.trainingLoss\n",
    "x = np.arange(1, len(values) + 1)\n",
    "\n",
    "plotLineGraph(\"Iterations\", \"Loss\", \"Loss value as a function of iteration\",x,values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42612d8d",
   "metadata": {},
   "source": [
    "#### Prediction Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec392f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted weights  [[0.34816665 0.47744879 0.28643378 0.47872592 0.21060643]\n",
      " [0.35705578 0.38888568 0.55408094 0.28422686 0.58802715]]  bias  [ 0.10765007 -0.07328381 -0.03874138  0.00575671 -0.0013816 ]\n",
      "Prediction loss 1.5124068221642941\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted weights \", model.weight, \" bias \", model.bias)\n",
    "\n",
    "actual     = testing_labels            # full y-vector, shape (n_samples,)\n",
    "probs, pred_oh = model.predict(testing_attributes, predictOneHot=True)\n",
    "loss = model.calculateCrossEntropyLoss(probs, pred_oh)\n",
    "\n",
    "\n",
    "print(\"Prediction loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b492f8d4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
